{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-23T18:51:16.107071Z",
     "iopub.status.busy": "2025-05-23T18:51:16.106824Z",
     "iopub.status.idle": "2025-05-23T18:51:29.292582Z",
     "shell.execute_reply": "2025-05-23T18:51:29.291737Z",
     "shell.execute_reply.started": "2025-05-23T18:51:16.107049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # Cell 1: Environment Setup and Imports\n",
    "# Install libraries (if needed) and import all required modules\n",
    "\n",
    "# Install required packages (run this if packages are missing)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"transformers>=4.36.0\",\n",
    "    \"datasets>=2.14.0\", \n",
    "    \"torch>=2.0.0\",\n",
    "    \"accelerate>=0.24.0\",\n",
    "    \"flash-attn>=2.3.0\",\n",
    "    \"wandb\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"numpy\",\n",
    "    \"tqdm\"\n",
    "]\n",
    "\n",
    "# Install packages if needed (uncomment if running first time)\n",
    "# for package in required_packages:\n",
    "#     install_package(package.split(\">=\")[0])\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Transformers and datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T18:51:38.749681Z",
     "iopub.status.busy": "2025-05-23T18:51:38.748820Z",
     "iopub.status.idle": "2025-05-23T18:51:51.449193Z",
     "shell.execute_reply": "2025-05-23T18:51:51.448319Z",
     "shell.execute_reply.started": "2025-05-23T18:51:38.749655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: GPU Check and Random Seed Setup\n",
    "# Log GPU info, number of GPUs, and set seeds for reproducibility\n",
    "\n",
    "def setup_device_and_seed(seed=42):\n",
    "    \"\"\"Setup device, log GPU info, and set random seeds\"\"\"\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Ensure deterministic behavior (may slow down training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(\"üîß Device and Seed Setup\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"‚úÖ CUDA is available!\")\n",
    "        print(f\"üì± Number of GPUs: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "            print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "            \n",
    "        # Display current GPU memory\n",
    "        print(f\"\\nüíæ Current GPU Memory Usage:\")\n",
    "        for i in range(num_gpus):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "            cached = torch.cuda.memory_reserved(i) / 1e9\n",
    "            print(f\"   GPU {i}: {allocated:.2f} GB allocated, {cached:.2f} GB cached\")\n",
    "            \n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        num_gpus = 0\n",
    "        print(\"‚ö†Ô∏è  CUDA not available, using CPU\")\n",
    "        \n",
    "    print(f\"\\nüé≤ Random seed set to: {seed}\")\n",
    "    print(f\"üñ•Ô∏è  Primary device: {device}\")\n",
    "    \n",
    "    return device, num_gpus\n",
    "\n",
    "# Setup device and seeds\n",
    "SEED = 42\n",
    "device, num_gpus = setup_device_and_seed(SEED)\n",
    "\n",
    "# Training configuration\n",
    "MULTI_GPU = num_gpus > 1\n",
    "print(f\"\\nüîÑ Multi-GPU training: {'Enabled' if MULTI_GPU else 'Disabled'}\")\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU cache cleared\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T18:52:03.018173Z",
     "iopub.status.busy": "2025-05-23T18:52:03.017553Z",
     "iopub.status.idle": "2025-05-23T18:52:14.339279Z",
     "shell.execute_reply": "2025-05-23T18:52:14.338429Z",
     "shell.execute_reply.started": "2025-05-23T18:52:03.018151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Load and Explore Dataset (Cosmopedia 100k)\n",
    "# Load from Hugging Face, print structure, show sample rows, and verify cleanliness\n",
    "\n",
    "def load_and_explore_dataset():\n",
    "    \"\"\"Load Cosmopedia 100k dataset and explore its structure\"\"\"\n",
    "    \n",
    "    print(\"üìö Loading Cosmopedia 100k Dataset\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    try:\n",
    "        ds = load_dataset(\"HuggingFaceTB/cosmopedia-100k\", split=\"train\")\n",
    "        print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Basic dataset info\n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"   Total samples: {len(ds):,}\")\n",
    "    print(f\"   Features: {list(ds.features.keys())}\")\n",
    "    print(f\"   Dataset size: {ds.data.nbytes / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Examine dataset structure\n",
    "    print(f\"\\nüîç Dataset Schema:\")\n",
    "    for feature_name, feature_type in ds.features.items():\n",
    "        print(f\"   {feature_name}: {feature_type}\")\n",
    "    \n",
    "    # Show sample rows\n",
    "    print(f\"\\nüìù Sample Rows (first 3):\")\n",
    "    for i in range(min(3, len(ds))):\n",
    "        sample = ds[i]\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, str):\n",
    "                # Truncate long text for display\n",
    "                display_value = value[:200] + \"...\" if len(value) > 200 else value\n",
    "                print(f\"   {key}: {display_value}\")\n",
    "            else:\n",
    "                print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Analyze text lengths\n",
    "    print(f\"\\nüìè Text Length Analysis:\")\n",
    "    text_lengths = [len(sample['text']) for sample in ds.select(range(min(1000, len(ds))))]\n",
    "    \n",
    "    print(f\"   Average text length: {np.mean(text_lengths):.0f} characters\")\n",
    "    print(f\"   Median text length: {np.median(text_lengths):.0f} characters\")\n",
    "    print(f\"   Min text length: {min(text_lengths):,} characters\")\n",
    "    print(f\"   Max text length: {max(text_lengths):,} characters\")\n",
    "    \n",
    "    # Check for any obvious data quality issues\n",
    "    print(f\"\\nüîç Data Quality Check:\")\n",
    "    empty_texts = sum(1 for sample in ds.select(range(min(1000, len(ds)))) if not sample['text'].strip())\n",
    "    print(f\"   Empty texts (in first 1000): {empty_texts}\")\n",
    "    \n",
    "    # Plot text length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(text_lengths, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Text Lengths in Cosmopedia 100k (Sample)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Load and explore the dataset\n",
    "dataset = load_and_explore_dataset()\n",
    "\n",
    "# Store dataset info for later use\n",
    "if dataset:\n",
    "    DATASET_SIZE = len(dataset)\n",
    "    print(f\"\\n‚úÖ Dataset ready for preprocessing! Total samples: {DATASET_SIZE:,}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load dataset. Please check your connection and try again.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T18:52:26.405956Z",
     "iopub.status.busy": "2025-05-23T18:52:26.405303Z",
     "iopub.status.idle": "2025-05-23T18:52:29.960092Z",
     "shell.execute_reply": "2025-05-23T18:52:29.959248Z",
     "shell.execute_reply.started": "2025-05-23T18:52:26.405930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Tokenizer - Existing Tokenizer\n",
    "# Load the pre-trained SmolLM2 tokenizer\n",
    "\n",
    "def load_smollm2_tokenizer():\n",
    "    \"\"\"Load the existing SmolLM2 tokenizer\"\"\"\n",
    "    \n",
    "    print(\"üî§ Loading SmolLM2 Tokenizer\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # SmolLM2 model ID\n",
    "    model_id = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        print(f\"‚úÖ Tokenizer loaded successfully from: {model_id}\")\n",
    "        \n",
    "        # Add padding token if it doesn't exist\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(\"üîß Set pad_token to eos_token\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Tokenizer information\n",
    "    print(f\"\\nüìä Tokenizer Information:\")\n",
    "    print(f\"   Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "    print(f\"   Model max length: {tokenizer.model_max_length:,}\")\n",
    "    print(f\"   Padding side: {tokenizer.padding_side}\")\n",
    "    print(f\"   Truncation side: {tokenizer.truncation_side}\")\n",
    "    \n",
    "    # Special tokens\n",
    "    print(f\"\\nüîë Special Tokens:\")\n",
    "    special_tokens = {\n",
    "        'bos_token': tokenizer.bos_token,\n",
    "        'eos_token': tokenizer.eos_token,\n",
    "        'pad_token': tokenizer.pad_token,\n",
    "        'unk_token': tokenizer.unk_token,\n",
    "    }\n",
    "    \n",
    "    for token_name, token_value in special_tokens.items():\n",
    "        token_id = getattr(tokenizer, f\"{token_name}_id\", None)\n",
    "        print(f\"   {token_name}: '{token_value}' (ID: {token_id})\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    print(f\"\\nüß™ Tokenization Test:\")\n",
    "    test_text = \"Hello, this is a test sentence for SmolLM2 tokenizer.\"\n",
    "    tokens = tokenizer.encode(test_text)\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    \n",
    "    print(f\"   Original: {test_text}\")\n",
    "    print(f\"   Tokens: {tokens[:10]}... (showing first 10)\")\n",
    "    print(f\"   Token count: {len(tokens)}\")\n",
    "    print(f\"   Decoded: {decoded}\")\n",
    "    \n",
    "    # Check tokenizer speed on a sample\n",
    "    print(f\"\\n‚ö° Speed Test:\")\n",
    "    test_texts = [\"This is a test sentence.\"] * 100\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tokenized = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"   Tokenized 100 sentences in {(end_time - start_time)*1000:.2f}ms\")\n",
    "    print(f\"   Batch shape: {tokenized['input_ids'].shape}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = load_smollm2_tokenizer()\n",
    "\n",
    "if tokenizer:\n",
    "    # Set sequence length for training\n",
    "    MAX_SEQ_LENGTH = 1024  # Adjust based on your needs and GPU memory\n",
    "    print(f\"\\nüìè Max sequence length for training: {MAX_SEQ_LENGTH}\")\n",
    "    print(\"‚úÖ Tokenizer ready for dataset preprocessing!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load tokenizer. Please check your connection and model ID.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T18:52:41.863687Z",
     "iopub.status.busy": "2025-05-23T18:52:41.863170Z",
     "iopub.status.idle": "2025-05-23T18:52:42.135466Z",
     "shell.execute_reply": "2025-05-23T18:52:42.134831Z",
     "shell.execute_reply.started": "2025-05-23T18:52:41.863664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Dataset Preprocessing - ULTRA OPTIMIZED (Minimal Memory)\n",
    "# Ultra-aggressive memory optimization to prevent checkpoint corruption\n",
    "\n",
    "import os\n",
    "# Fix tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CosmopediaDataset(Dataset):\n",
    "    \"\"\"Ultra Memory-Optimized Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer, max_length=256):  # Further reduced\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get text from dataset\n",
    "        text = self.dataset[idx]['text']\n",
    "        \n",
    "        # Tokenize with aggressive truncation\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract input_ids and attention_mask\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # For causal language modeling, labels are the same as input_ids\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def preprocess_dataset_ultra_optimized(dataset, tokenizer, max_length=256, batch_size=1):\n",
    "    \"\"\"Ultra memory-optimized preprocessing\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Dataset Preprocessing - ULTRA OPTIMIZED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if dataset is None or tokenizer is None:\n",
    "        print(\"‚ùå Dataset or tokenizer is None. Please load them first.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"üìä ULTRA OPTIMIZED Configuration:\")\n",
    "    print(f\"   Max sequence length: {max_length} (ultra-reduced)\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Total samples: {len(dataset):,}\")\n",
    "    print(f\"   Memory optimization: ULTRA AGGRESSIVE\")\n",
    "    \n",
    "    # Create custom dataset\n",
    "    print(f\"\\nüîÑ Creating PyTorch Dataset...\")\n",
    "    torch_dataset = CosmopediaDataset(dataset, tokenizer, max_length)\n",
    "    \n",
    "    # Test a single sample\n",
    "    print(f\"\\nüß™ Testing Dataset Sample:\")\n",
    "    sample = torch_dataset[0]\n",
    "    print(f\"   Input IDs shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"   Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "    print(f\"   Labels shape: {sample['labels'].shape}\")\n",
    "    \n",
    "    # Create DataLoader with ultra optimization\n",
    "    print(f\"\\nüöÄ Creating Ultra-Optimized DataLoader...\")\n",
    "    dataloader = DataLoader(\n",
    "        torch_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # No multiprocessing\n",
    "        pin_memory=False,  # Disabled\n",
    "        drop_last=True,\n",
    "        persistent_workers=False  # Disable persistent workers\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ DataLoader created successfully!\")\n",
    "    print(f\"   Number of batches: {len(dataloader):,}\")\n",
    "    \n",
    "    # Test DataLoader with memory monitoring\n",
    "    print(f\"\\nüß™ Testing DataLoader:\")\n",
    "    try:\n",
    "        # Clear cache before testing\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        test_batch = next(iter(dataloader))\n",
    "        print(f\"   Batch input_ids shape: {test_batch['input_ids'].shape}\")\n",
    "        \n",
    "        # Check memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            batch_memory = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"   GPU memory after batch: {batch_memory:.2f} GB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error testing DataLoader: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    return torch_dataset, dataloader\n",
    "\n",
    "# ULTRA OPTIMIZED Configuration for preventing memory corruption\n",
    "BATCH_SIZE = 1  # Keep at 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 64  # Increased to maintain effective batch size\n",
    "MAX_SEQ_LENGTH_ULTRA = 256  # Reduced from 512 to 256\n",
    "\n",
    "# Calculate effective batch size\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * max(1, num_gpus)\n",
    "\n",
    "print(f\"üîß ULTRA OPTIMIZED Configuration:\")\n",
    "print(f\"   Batch size per GPU: {BATCH_SIZE}\")\n",
    "print(f\"   Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   Number of GPUs: {num_gpus}\")\n",
    "print(f\"   Effective batch size: {EFFECTIVE_BATCH_SIZE}\")\n",
    "print(f\"   Sequence length: {MAX_SEQ_LENGTH_ULTRA}\")\n",
    "print(f\"   Target GPU usage: <12GB (safe zone)\")\n",
    "\n",
    "# Clear GPU memory aggressively\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"üßπ GPU cache cleared and synchronized\")\n",
    "\n",
    "# Preprocess dataset with ultra settings\n",
    "train_dataset, train_dataloader = preprocess_dataset_ultra_optimized(\n",
    "    dataset, \n",
    "    tokenizer, \n",
    "    max_length=MAX_SEQ_LENGTH_ULTRA, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "if train_dataloader:\n",
    "    # Calculate final training statistics\n",
    "    STEPS_PER_EPOCH = len(train_dataloader)\n",
    "    TOTAL_STEPS = STEPS_PER_EPOCH * 3  # 3 epochs\n",
    "    \n",
    "    print(f\"\\n‚úÖ ULTRA OPTIMIZED Preprocessing Complete!\")\n",
    "    print(f\"üìä Final Training Statistics:\")\n",
    "    print(f\"   Steps per epoch: {STEPS_PER_EPOCH:,}\")\n",
    "    print(f\"   Total training steps (3 epochs): {TOTAL_STEPS:,}\")\n",
    "    print(f\"   Samples per effective step: {EFFECTIVE_BATCH_SIZE}\")\n",
    "    print(f\"   Memory footprint: MINIMIZED\")\n",
    "    print(f\"   Expected GPU usage: ~10-12GB (safe)\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create DataLoader. Please check your setup.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T18:52:57.558377Z",
     "iopub.status.busy": "2025-05-23T18:52:57.557802Z",
     "iopub.status.idle": "2025-05-23T18:54:18.688283Z",
     "shell.execute_reply": "2025-05-23T18:54:18.687524Z",
     "shell.execute_reply.started": "2025-05-23T18:52:57.558349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Model Configuration and Initialization (NO MIXED PRECISION VERSION)\n",
    "# Define model config, optimizer, scheduler, gradient clipping setup WITHOUT mixed precision\n",
    "\n",
    "import os\n",
    "\n",
    "# Set memory optimization environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def setup_model_and_training_components_no_amp():\n",
    "    \"\"\"Initialize SmolLM2 model with random weights and setup training components - NO MIXED PRECISION\"\"\"\n",
    "    \n",
    "    print(\"ü§ñ Model Configuration and Initialization (NO MIXED PRECISION)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Clear GPU cache before model initialization\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ GPU cache cleared before model setup\")\n",
    "    \n",
    "    # Load SmolLM2 configuration\n",
    "    model_id = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "    \n",
    "    try:\n",
    "        # Load config only (not weights)\n",
    "        config = AutoConfig.from_pretrained(model_id)\n",
    "        print(f\"‚úÖ Config loaded from: {model_id}\")\n",
    "        \n",
    "        # MEMORY OPTIMIZATION: Enable gradient checkpointing\n",
    "        config.use_cache = False  # Disable KV cache to save memory\n",
    "        config.gradient_checkpointing = True\n",
    "        \n",
    "        # Display model configuration\n",
    "        print(f\"\\nüìä Model Configuration:\")\n",
    "        print(f\"   Model type: {config.model_type}\")\n",
    "        print(f\"   Hidden size: {config.hidden_size:,}\")\n",
    "        print(f\"   Number of layers: {config.num_hidden_layers}\")\n",
    "        print(f\"   Number of attention heads: {config.num_attention_heads}\")\n",
    "        print(f\"   Vocabulary size: {config.vocab_size:,}\")\n",
    "        print(f\"   Max position embeddings: {config.max_position_embeddings:,}\")\n",
    "        print(f\"   Use cache: {config.use_cache} (disabled for memory)\")\n",
    "        print(f\"   Gradient checkpointing: {config.gradient_checkpointing}\")\n",
    "        \n",
    "        # Calculate approximate model parameters\n",
    "        approx_params = (config.vocab_size * config.hidden_size +  # Embedding\n",
    "                        config.num_hidden_layers * (\n",
    "                            4 * config.hidden_size * config.hidden_size +  # MLP\n",
    "                            3 * config.hidden_size * config.hidden_size    # Attention\n",
    "                        ) +\n",
    "                        config.vocab_size * config.hidden_size)  # Output layer\n",
    "        \n",
    "        print(f\"   Approximate parameters: {approx_params / 1e9:.2f}B\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading config: {e}\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Enable Flash Attention 2 if available (memory efficient)\n",
    "    try:\n",
    "        config.use_flash_attention_2 = True\n",
    "        print(f\"üöÄ Flash Attention 2: Enabled (memory efficient)\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Flash Attention 2: Not available, using standard attention\")\n",
    "    \n",
    "    # Initialize model with random weights\n",
    "    print(f\"\\nüé≤ Initializing model with random weights...\")\n",
    "    try:\n",
    "        # Create model with memory optimizations\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        \n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(f\"‚úÖ Gradient checkpointing enabled\")\n",
    "        \n",
    "        print(f\"‚úÖ Model initialized successfully!\")\n",
    "        \n",
    "        # Count actual parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\nüìà Model Parameters:\")\n",
    "        print(f\"   Total parameters: {total_params / 1e9:.2f}B ({total_params:,})\")\n",
    "        print(f\"   Trainable parameters: {trainable_params / 1e9:.2f}B ({trainable_params:,})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing model: {e}\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    print(f\"üì± Model moved to: {device}\")\n",
    "    \n",
    "    # MEMORY CHECK after model loading\n",
    "    if torch.cuda.is_available():\n",
    "        model_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"üíæ GPU memory after model loading: {model_memory:.2f} GB\")\n",
    "    \n",
    "    # Setup optimizer (AdamW)\n",
    "    print(f\"\\n‚öôÔ∏è  Setting up optimizer...\")\n",
    "    \n",
    "    learning_rate = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    # No weight decay for bias and layer norm parameters\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.95),\n",
    "        eps=1e-8,\n",
    "        foreach=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ AdamW optimizer configured:\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    print(f\"   Weight decay: {weight_decay}\")\n",
    "    \n",
    "    # Setup learning rate scheduler\n",
    "    print(f\"\\nüìà Setting up learning rate scheduler...\")\n",
    "    \n",
    "    num_warmup_steps = int(0.1 * TOTAL_STEPS)  # 10% warmup\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=TOTAL_STEPS\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Cosine scheduler with warmup configured:\")\n",
    "    print(f\"   Warmup steps: {num_warmup_steps:,}\")\n",
    "    print(f\"   Total steps: {TOTAL_STEPS:,}\")\n",
    "    \n",
    "    # NO MIXED PRECISION - Use None for scaler\n",
    "    print(f\"\\nüéØ Training setup (NO MIXED PRECISION):\")\n",
    "    scaler = None  # No scaler needed for FP32 training\n",
    "    print(f\"‚úÖ Mixed precision: DISABLED (using FP32 for compatibility)\")\n",
    "    print(f\"   Memory usage: Higher but more stable\")\n",
    "    print(f\"   Compatibility: Maximum (no precision issues)\")\n",
    "    \n",
    "    # Training configuration summary\n",
    "    print(f\"\\nüéõÔ∏è  Training Configuration Summary:\")\n",
    "    print(f\"   Mixed precision: DISABLED (FP32 training)\")\n",
    "    print(f\"   Gradient checkpointing: ENABLED\")\n",
    "    print(f\"   Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"   Max gradient norm: 1.0 (for clipping)\")\n",
    "    print(f\"   Effective batch size: {EFFECTIVE_BATCH_SIZE}\")\n",
    "    print(f\"   Precision: FP32 (maximum compatibility)\")\n",
    "    \n",
    "    # Final memory check\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        final_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"\\nüíæ Final GPU memory usage: {final_memory:.2f} GB\")\n",
    "        free_memory = (torch.cuda.get_device_properties(0).total_memory / 1e9) - final_memory\n",
    "        print(f\"üíæ Available GPU memory: {free_memory:.2f} GB\")\n",
    "    \n",
    "    return model, optimizer, scheduler, scaler, config\n",
    "\n",
    "# Initialize model and training components WITHOUT mixed precision\n",
    "print(\"üöÄ Initializing training components (NO MIXED PRECISION)...\")\n",
    "model, optimizer, scheduler, scaler, model_config = setup_model_and_training_components_no_amp()\n",
    "\n",
    "if model is not None:\n",
    "    print(f\"\\n‚úÖ Model and training components ready!\")\n",
    "    \n",
    "    # Final memory report\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        print(f\"\\nüíæ FINAL GPU Memory Report:\")\n",
    "        print(f\"   Total GPU memory: {total_memory:.2f} GB\")\n",
    "        print(f\"   Allocated: {memory_allocated:.2f} GB ({memory_allocated/total_memory*100:.1f}%)\")\n",
    "        print(f\"   Reserved: {memory_reserved:.2f} GB ({memory_reserved/total_memory*100:.1f}%)\")\n",
    "        print(f\"   Free: {total_memory - memory_reserved:.2f} GB\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize model. Please check your setup.\")\n",
    "\n",
    "# Gradient clipping value\n",
    "MAX_GRAD_NORM = 1.0\n",
    "print(f\"\\nüîß Gradient clipping norm: {MAX_GRAD_NORM}\")\n",
    "print(f\"\\nüí° Note: This version uses FP32 training for maximum compatibility\")\n",
    "print(f\"   Memory usage will be higher but training should be stable\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T18:54:28.581295Z",
     "iopub.status.busy": "2025-05-23T18:54:28.580745Z",
     "iopub.status.idle": "2025-05-23T18:58:15.636815Z",
     "shell.execute_reply": "2025-05-23T18:58:15.636160Z",
     "shell.execute_reply.started": "2025-05-23T18:54:28.581273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Ultra-Safe Training Loop with Robust Checkpointing\n",
    "# Ultra-aggressive memory management and corruption-resistant checkpointing\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Set environment variables for maximum stability\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Training tracking variables\n",
    "training_stats = {\n",
    "    'steps': [],\n",
    "    'losses': [],\n",
    "    'learning_rates': [],\n",
    "    'epochs': [],\n",
    "    'gpu_memory': []\n",
    "}\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Ultra-aggressive memory cleanup\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()  # Python garbage collection\n",
    "\n",
    "def safe_checkpoint_save(model, optimizer, scheduler, step, epoch, loss, checkpoint_dir=\"./checkpoints\"):\n",
    "    \"\"\"Ultra-safe checkpoint saving with corruption prevention\"\"\"\n",
    "    \n",
    "    Path(checkpoint_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Aggressive memory cleanup before saving\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'training_stats': training_stats\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = f\"{checkpoint_dir}/checkpoint_step_{step}.pt\"\n",
    "    temp_path = f\"{checkpoint_dir}/temp_checkpoint_step_{step}.pt\"\n",
    "    \n",
    "    try:\n",
    "        # Save to temporary file first\n",
    "        torch.save(checkpoint, temp_path)\n",
    "        \n",
    "        # Verify the file was written correctly\n",
    "        test_load = torch.load(temp_path, map_location='cpu')\n",
    "        if 'step' in test_load and test_load['step'] == step:\n",
    "            # Move temp file to final location\n",
    "            shutil.move(temp_path, checkpoint_path)\n",
    "            print(f\"üíæ Checkpoint saved safely: {checkpoint_path}\")\n",
    "        else:\n",
    "            raise Exception(\"Checkpoint verification failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Checkpoint save failed: {e}\")\n",
    "        # Clean up temp file\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        return None\n",
    "    \n",
    "    # Final cleanup after saving\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint_safe(checkpoint_path, model, optimizer, scheduler):\n",
    "    \"\"\"Safe checkpoint loading with verification\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load and verify checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        if 'model_state_dict' not in checkpoint:\n",
    "            raise Exception(\"Invalid checkpoint format\")\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        start_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['loss']\n",
    "        \n",
    "        global training_stats\n",
    "        training_stats = checkpoint.get('training_stats', training_stats)\n",
    "        \n",
    "        print(f\"üìÇ Checkpoint loaded successfully: {checkpoint_path}\")\n",
    "        print(f\"   Resuming from step: {start_step}\")\n",
    "        print(f\"   Previous loss: {best_loss:.4f}\")\n",
    "        \n",
    "        return start_step, start_epoch, best_loss\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading checkpoint: {e}\")\n",
    "        return 0, 0, float('inf')\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir=\"./checkpoints\"):\n",
    "    \"\"\"Find the latest valid checkpoint\"\"\"\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "        \n",
    "    checkpoints = glob.glob(f\"{checkpoint_dir}/checkpoint_step_*.pt\")\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "        \n",
    "    # Sort by step number and validate\n",
    "    valid_checkpoints = []\n",
    "    for cp in checkpoints:\n",
    "        try:\n",
    "            # Quick validation\n",
    "            test = torch.load(cp, map_location='cpu')\n",
    "            if 'step' in test:\n",
    "                valid_checkpoints.append(cp)\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Removing corrupted checkpoint: {cp}\")\n",
    "            os.remove(cp)\n",
    "    \n",
    "    if not valid_checkpoints:\n",
    "        return None\n",
    "        \n",
    "    valid_checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    return valid_checkpoints[-1]\n",
    "\n",
    "def train_model_ultra_safe():\n",
    "    \"\"\"Ultra-safe training with aggressive memory management\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting ULTRA-SAFE Training Loop\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initial aggressive cleanup\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    # Check for existing checkpoints\n",
    "    latest_checkpoint = find_latest_checkpoint()\n",
    "    start_step = 0\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    if latest_checkpoint:\n",
    "        response = input(f\"Found checkpoint: {latest_checkpoint}. Resume? (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            start_step, start_epoch, best_loss = load_checkpoint_safe(\n",
    "                latest_checkpoint, model, optimizer, scheduler\n",
    "            )\n",
    "    \n",
    "    # Calculate starting position\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "    current_epoch = start_step // steps_per_epoch\n",
    "    current_step_in_epoch = start_step % steps_per_epoch\n",
    "    \n",
    "    print(f\"\\nüéØ ULTRA-SAFE Training Configuration:\")\n",
    "    print(f\"   Starting from step: {start_step}\")\n",
    "    print(f\"   Total epochs: 3\")\n",
    "    print(f\"   Steps per epoch: {steps_per_epoch:,}\")\n",
    "    print(f\"   Total steps: {TOTAL_STEPS:,}\")\n",
    "    print(f\"   Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"   Effective batch size: {EFFECTIVE_BATCH_SIZE}\")\n",
    "    print(f\"   Sequence length: {MAX_SEQ_LENGTH_ULTRA}\")\n",
    "    print(f\"   Memory management: ULTRA AGGRESSIVE\")\n",
    "    print(f\"   Checkpoint frequency: Every 200 steps (safer)\")\n",
    "    \n",
    "    # Training loop with ultra-safe memory management\n",
    "    model.train()\n",
    "    global_step = start_step\n",
    "    running_loss = 0.0\n",
    "    log_interval = 10\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(start_epoch, 3):\n",
    "            print(f\"\\nüîÑ Epoch {epoch + 1}/3\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            epoch_start_time = time.time()\n",
    "            epoch_loss = 0.0\n",
    "            epoch_steps = 0\n",
    "            \n",
    "            # Ultra-aggressive cleanup at epoch start\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Skip batches if resuming mid-epoch\n",
    "            dataloader_iter = iter(train_dataloader)\n",
    "            for _ in range(current_step_in_epoch):\n",
    "                next(dataloader_iter)\n",
    "            \n",
    "            if epoch > start_epoch:\n",
    "                current_step_in_epoch = 0\n",
    "                dataloader_iter = iter(train_dataloader)\n",
    "            \n",
    "            progress_bar = tqdm(\n",
    "                dataloader_iter, \n",
    "                total=steps_per_epoch - current_step_in_epoch,\n",
    "                desc=f\"Epoch {epoch+1}\",\n",
    "                initial=current_step_in_epoch\n",
    "            )\n",
    "            \n",
    "            for step, batch in enumerate(progress_bar, start=current_step_in_epoch):\n",
    "                step_start_time = time.time()\n",
    "                \n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                labels = batch['labels'].to(device, non_blocking=True)\n",
    "                \n",
    "                # Forward pass (FP32)\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    use_cache=False\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                current_loss = loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "                \n",
    "                # Gradient accumulation and optimization\n",
    "                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Memory cleanup every optimizer step\n",
    "                    if global_step % 10 == 0:\n",
    "                        aggressive_memory_cleanup()\n",
    "                \n",
    "                # Update statistics\n",
    "                running_loss += current_loss\n",
    "                epoch_loss += current_loss\n",
    "                epoch_steps += 1\n",
    "                global_step += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if global_step % log_interval == 0:\n",
    "                    avg_loss = running_loss / log_interval\n",
    "                    current_lr = scheduler.get_last_lr()[0]\n",
    "                    step_time = time.time() - step_start_time\n",
    "                    \n",
    "                    # GPU memory usage\n",
    "                    gpu_memory = 0\n",
    "                    if torch.cuda.is_available():\n",
    "                        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "                    \n",
    "                    # Store statistics\n",
    "                    training_stats['steps'].append(global_step)\n",
    "                    training_stats['losses'].append(avg_loss)\n",
    "                    training_stats['learning_rates'].append(current_lr)\n",
    "                    training_stats['epochs'].append(epoch + 1)\n",
    "                    training_stats['gpu_memory'].append(gpu_memory)\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({\n",
    "                        'Loss': f'{avg_loss:.4f}',\n",
    "                        'LR': f'{current_lr:.2e}',\n",
    "                        'GPU': f'{gpu_memory:.1f}GB',\n",
    "                        'Time': f'{step_time:.2f}s'\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"\\nüìä Step {global_step:,}/{TOTAL_STEPS:,} | \"\n",
    "                          f\"Loss: {avg_loss:.4f} | \"\n",
    "                          f\"LR: {current_lr:.2e} | \"\n",
    "                          f\"GPU: {gpu_memory:.1f}GB\")\n",
    "                    \n",
    "                    running_loss = 0.0\n",
    "                \n",
    "                # SAFER checkpointing - every 200 steps instead of 100\n",
    "                if global_step % 200 == 0:\n",
    "                    print(f\"\\nüíæ Saving checkpoint at step {global_step}...\")\n",
    "                    checkpoint_path = safe_checkpoint_save(\n",
    "                        model, optimizer, scheduler, \n",
    "                        global_step, epoch, current_loss\n",
    "                    )\n",
    "                    \n",
    "                    if checkpoint_path:\n",
    "                        # Keep only last 2 checkpoints to save space\n",
    "                        checkpoints = glob.glob(\"./checkpoints/checkpoint_step_*.pt\")\n",
    "                        if len(checkpoints) > 2:\n",
    "                            checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "                            for old_checkpoint in checkpoints[:-2]:\n",
    "                                os.remove(old_checkpoint)\n",
    "                                print(f\"üóëÔ∏è Removed old checkpoint: {old_checkpoint}\")\n",
    "                \n",
    "                # Memory monitoring with warnings\n",
    "                if global_step % 100 == 0:\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "                        if memory_used > 12:\n",
    "                            print(f\"‚ö†Ô∏è Memory usage: {memory_used:.2f}GB - performing cleanup\")\n",
    "                            aggressive_memory_cleanup()\n",
    "                \n",
    "                # Early stopping check\n",
    "                if global_step >= TOTAL_STEPS:\n",
    "                    print(f\"\\nüéØ Reached maximum steps ({TOTAL_STEPS})\")\n",
    "                    break\n",
    "            \n",
    "            # Epoch summary\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else 0\n",
    "            \n",
    "            print(f\"\\nüìà Epoch {epoch + 1} Summary:\")\n",
    "            print(f\"   Average Loss: {avg_epoch_loss:.4f}\")\n",
    "            print(f\"   Time: {epoch_time / 60:.2f} minutes\")\n",
    "            print(f\"   Steps: {epoch_steps}\")\n",
    "            print(f\"   Global Step: {global_step:,}/{TOTAL_STEPS:,}\")\n",
    "            \n",
    "            # Save end-of-epoch checkpoint\n",
    "            safe_checkpoint_save(\n",
    "                model, optimizer, scheduler, \n",
    "                global_step, epoch + 1, avg_epoch_loss\n",
    "            )\n",
    "            \n",
    "            if global_step >= TOTAL_STEPS:\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "        safe_checkpoint_save(model, optimizer, scheduler, global_step, epoch, current_loss)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training error: {e}\")\n",
    "        print(f\"üìä Error occurred at step: {global_step}\")\n",
    "        print(f\"üíæ Attempting emergency checkpoint save...\")\n",
    "        safe_checkpoint_save(model, optimizer, scheduler, global_step, epoch, current_loss)\n",
    "        raise\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed!\")\n",
    "    print(f\"   Total steps: {global_step:,}\")\n",
    "    print(f\"   Final loss: {training_stats['losses'][-1] if training_stats['losses'] else 'N/A'}\")\n",
    "    \n",
    "    return training_stats\n",
    "\n",
    "# Check if all components are ready\n",
    "missing_components = []\n",
    "if 'model' not in globals() or model is None: missing_components.append(\"model\")\n",
    "if 'optimizer' not in globals() or optimizer is None: missing_components.append(\"optimizer\") \n",
    "if 'scheduler' not in globals() or scheduler is None: missing_components.append(\"scheduler\")\n",
    "if 'train_dataloader' not in globals() or train_dataloader is None: missing_components.append(\"dataloader\")\n",
    "\n",
    "if not missing_components:\n",
    "    print(\"üîç All components ready for ULTRA-SAFE training!\")\n",
    "    print(\"üöÄ Starting ultra-safe training with corruption prevention...\")\n",
    "    \n",
    "    # Ultra-aggressive initial cleanup\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"üíæ Initial GPU memory: {initial_memory:.2f} GB\")\n",
    "        print(f\"üéØ Target: Keep under 12GB to prevent corruption\")\n",
    "    \n",
    "    try:\n",
    "        training_stats = train_model_ultra_safe()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training cancelled by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        print(\"üí° Try restarting kernel if corruption persists\")\n",
    "else:\n",
    "    print(\"‚ùå Some components are missing. Please run previous cells first.\")\n",
    "    print(f\"Missing: {', '.join(missing_components)}\")\n",
    "    print(\"\\nüîß This ultra-safe version should prevent checkpoint corruption\")\n",
    "    print(\"   - Reduced sequence length: 256\")\n",
    "    print(\"   - Safer checkpointing: every 200 steps\")\n",
    "    print(\"   - Aggressive memory management\")\n",
    "    print(\"   - Corruption-resistant file operations\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: Loss Visualization and Statistics Logging\n",
    "# Plot training loss curves, log metrics, and save visualizations/artifacts\n",
    "\n",
    "def plot_training_metrics(training_stats, save_plots=True):\n",
    "    \"\"\"Plot training metrics and statistics\"\"\"\n",
    "    \n",
    "    print(\"üìä Visualizing Training Metrics\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not training_stats['steps']:\n",
    "        print(\"‚ùå No training statistics available. Please run training first.\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('SmolLM2 Training Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Training Loss\n",
    "    axes[0, 0].plot(training_stats['steps'], training_stats['losses'], 'b-', linewidth=2, alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Training Step')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss Over Time')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')  # Log scale for better visualization\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(training_stats['steps']) > 10:\n",
    "        z = np.polyfit(training_stats['steps'], np.log(training_stats['losses']), 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[0, 0].plot(training_stats['steps'], np.exp(p(training_stats['steps'])), \n",
    "                       'r--', alpha=0.7, label='Trend')\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Learning Rate Schedule\n",
    "    axes[0, 1].plot(training_stats['steps'], training_stats['learning_rates'], 'g-', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Training Step')\n",
    "    axes[0, 1].set_ylabel('Learning Rate')\n",
    "    axes[0, 1].set_title('Learning Rate Schedule')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    \n",
    "    # 3. GPU Memory Usage\n",
    "    if training_stats['gpu_memory'] and any(mem > 0 for mem in training_stats['gpu_memory']):\n",
    "        axes[1, 0].plot(training_stats['steps'], training_stats['gpu_memory'], 'r-', linewidth=2)\n",
    "        axes[1, 0].set_xlabel('Training Step')\n",
    "        axes[1, 0].set_ylabel('GPU Memory (GB)')\n",
    "        axes[1, 0].set_title('GPU Memory Usage')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add average line\n",
    "        avg_memory = np.mean(training_stats['gpu_memory'])\n",
    "        axes[1, 0].axhline(y=avg_memory, color='orange', linestyle='--', \n",
    "                          label=f'Average: {avg_memory:.1f}GB')\n",
    "        axes[1, 0].legend()\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'GPU Memory\\nData Not Available', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes,\n",
    "                       fontsize=12)\n",
    "        axes[1, 0].set_title('GPU Memory Usage')\n",
    "    \n",
    "    # 4. Loss per Epoch\n",
    "    if training_stats['epochs']:\n",
    "        epochs = training_stats['epochs']\n",
    "        unique_epochs = sorted(set(epochs))\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for epoch in unique_epochs:\n",
    "            epoch_indices = [i for i, e in enumerate(epochs) if e == epoch]\n",
    "            if epoch_indices:\n",
    "                epoch_loss = np.mean([training_stats['losses'][i] for i in epoch_indices])\n",
    "                epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        if epoch_losses:\n",
    "            axes[1, 1].bar(unique_epochs, epoch_losses, alpha=0.7, color='purple')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Average Loss')\n",
    "            axes[1, 1].set_title('Average Loss per Epoch')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add values on bars\n",
    "            for i, v in enumerate(epoch_losses):\n",
    "                axes[1, 1].text(unique_epochs[i], v, f'{v:.3f}', \n",
    "                               ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"üìÅ Training metrics saved as 'training_metrics.png'\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def generate_training_report(training_stats):\n",
    "    \"\"\"Generate a comprehensive training report\"\"\"\n",
    "    \n",
    "    print(\"\\nüìã Training Report\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not training_stats['steps']:\n",
    "        print(\"‚ùå No training data available\")\n",
    "        return\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_steps = len(training_stats['steps'])\n",
    "    final_loss = training_stats['losses'][-1] if training_stats['losses'] else 0\n",
    "    initial_loss = training_stats['losses'][0] if training_stats['losses'] else 0\n",
    "    loss_reduction = ((initial_loss - final_loss) / initial_loss * 100) if initial_loss > 0 else 0\n",
    "    \n",
    "    print(f\"üìä Training Summary:\")\n",
    "    print(f\"   Total training steps: {training_stats['steps'][-1]:,}\")\n",
    "    print(f\"   Logged data points: {total_steps:,}\")\n",
    "    print(f\"   Initial loss: {initial_loss:.4f}\")\n",
    "    print(f\"   Final loss: {final_loss:.4f}\")\n",
    "    print(f\"   Loss reduction: {loss_reduction:.1f}%\")\n",
    "    \n",
    "    # Loss statistics\n",
    "    if training_stats['losses']:\n",
    "        min_loss = min(training_stats['losses'])\n",
    "        max_loss = max(training_stats['losses'])\n",
    "        avg_loss = np.mean(training_stats['losses'])\n",
    "        std_loss = np.std(training_stats['losses'])\n",
    "        \n",
    "        print(f\"\\nüìà Loss Statistics:\")\n",
    "        print(f\"   Minimum loss: {min_loss:.4f}\")\n",
    "        print(f\"   Maximum loss: {max_loss:.4f}\")\n",
    "        print(f\"   Average loss: {avg_loss:.4f}\")\n",
    "        print(f\"   Standard deviation: {std_loss:.4f}\")\n",
    "    \n",
    "    # Learning rate statistics\n",
    "    if training_stats['learning_rates']:\n",
    "        initial_lr = training_stats['learning_rates'][0]\n",
    "        final_lr = training_stats['learning_rates'][-1]\n",
    "        max_lr = max(training_stats['learning_rates'])\n",
    "        \n",
    "        print(f\"\\nüìà Learning Rate:\")\n",
    "        print(f\"   Initial LR: {initial_lr:.2e}\")\n",
    "        print(f\"   Final LR: {final_lr:.2e}\")\n",
    "        print(f\"   Maximum LR: {max_lr:.2e}\")\n",
    "    \n",
    "    # GPU Memory statistics\n",
    "    if training_stats['gpu_memory'] and any(mem > 0 for mem in training_stats['gpu_memory']):\n",
    "        avg_memory = np.mean(training_stats['gpu_memory'])\n",
    "        max_memory = max(training_stats['gpu_memory'])\n",
    "        min_memory = min(training_stats['gpu_memory'])\n",
    "        \n",
    "        print(f\"\\nüíæ GPU Memory Usage:\")\n",
    "        print(f\"   Average: {avg_memory:.2f} GB\")\n",
    "        print(f\"   Maximum: {max_memory:.2f} GB\")\n",
    "        print(f\"   Minimum: {min_memory:.2f} GB\")\n",
    "    \n",
    "    # Epoch statistics\n",
    "    if training_stats['epochs']:\n",
    "        completed_epochs = max(training_stats['epochs'])\n",
    "        print(f\"\\nüîÑ Training Progress:\")\n",
    "        print(f\"   Completed epochs: {completed_epochs}\")\n",
    "        print(f\"   Target epochs: 3\")\n",
    "        print(f\"   Progress: {completed_epochs/3*100:.1f}%\")\n",
    "    \n",
    "    # Training stability analysis\n",
    "    if len(training_stats['losses']) > 10:\n",
    "        recent_losses = training_stats['losses'][-10:]  # Last 10 losses\n",
    "        early_losses = training_stats['losses'][:10]    # First 10 losses\n",
    "        \n",
    "        recent_trend = np.polyfit(range(len(recent_losses)), recent_losses, 1)[0]\n",
    "        \n",
    "        print(f\"\\nüìâ Training Stability:\")\n",
    "        print(f\"   Recent trend: {'Decreasing' if recent_trend < 0 else 'Increasing'}\")\n",
    "        print(f\"   Trend slope: {recent_trend:.6f}\")\n",
    "        \n",
    "        # Convergence indicator\n",
    "        if abs(recent_trend) < 0.001:\n",
    "            print(f\"   Status: üü¢ Model appears to be converging\")\n",
    "        elif recent_trend < -0.01:\n",
    "            print(f\"   Status: üü° Model is still learning rapidly\")\n",
    "        else:\n",
    "            print(f\"   Status: üî¥ Model may be diverging or overfitting\")\n",
    "\n",
    "def save_training_stats(training_stats, filename='training_stats.json'):\n",
    "    \"\"\"Save training statistics to JSON file\"\"\"\n",
    "    \n",
    "    try:\n",
    "        import json\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(training_stats, f, indent=2)\n",
    "        print(f\"üíæ Training statistics saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving training stats: {e}\")\n",
    "\n",
    "# Check if training stats are available\n",
    "if 'training_stats' in globals() and training_stats['steps']:\n",
    "    print(\"üìä Training statistics found! Generating visualizations...\")\n",
    "    \n",
    "    # Plot training metrics\n",
    "    plot_training_metrics(training_stats)\n",
    "    \n",
    "    # Generate training report\n",
    "    generate_training_report(training_stats)\n",
    "    \n",
    "    # Save training statistics\n",
    "    save_training_stats(training_stats)\n",
    "    \n",
    "    print(\"\\n‚úÖ Training analysis complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No training statistics available.\")\n",
    "    print(\"Please run the training loop (Cell 7) first to generate data.\")\n",
    "    \n",
    "    # Create dummy data for demonstration (remove this in actual training)\n",
    "    print(\"\\nüîß Creating sample visualization for demonstration...\")\n",
    "    \n",
    "    dummy_stats = {\n",
    "        'steps': list(range(0, 1000, 10)),\n",
    "        'losses': [4.5 * np.exp(-i/500) + 0.5 + 0.1*np.random.random() for i in range(0, 1000, 10)],\n",
    "        'learning_rates': [5e-5 * (0.5 + 0.5*np.cos(i*np.pi/1000)) for i in range(0, 1000, 10)],\n",
    "        'epochs': [min(3, i//333 + 1) for i in range(0, 1000, 10)],\n",
    "        'gpu_memory': [8.5 + 0.5*np.random.random() for _ in range(100)]\n",
    "    }\n",
    "    \n",
    "    plot_training_metrics(dummy_stats, save_plots=False)\n",
    "    print(\"üìù This is sample data. Run training to see real metrics.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 9: Sample Generation and Output Check\n",
    "# Generate some text using the trained model to qualitatively assess performance\n",
    "\n",
    "def generate_text_samples(model, tokenizer, prompts=None, max_length=200, num_samples=3, temperature=0.7):\n",
    "    \"\"\"Generate text samples using the trained model\"\"\"\n",
    "    \n",
    "    print(\"üé≠ Text Generation - Model Performance Check\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"‚ùå Model or tokenizer not available. Please run training first.\")\n",
    "        return\n",
    "    \n",
    "    # Default prompts if none provided\n",
    "    if prompts is None:\n",
    "        prompts = [\n",
    "            \"The future of artificial intelligence\",\n",
    "            \"In a world where technology\",\n",
    "            \"Scientists have recently discovered\",\n",
    "            \"The most important lesson in life\",\n",
    "            \"Climate change is\"\n",
    "        ]\n",
    "    \n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"üéØ Generation Parameters:\")\n",
    "    print(f\"   Max length: {max_length} tokens\")\n",
    "    print(f\"   Temperature: {temperature}\")\n",
    "    print(f\"   Number of samples per prompt: {num_samples}\")\n",
    "    print(f\"   Total generations: {len(prompts) * num_samples}\")\n",
    "    \n",
    "    all_generations = []\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                print(f\"\\nüìù Prompt {i+1}: '{prompt}'\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                for sample_idx in range(num_samples):\n",
    "                    # Tokenize prompt\n",
    "                    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                    input_ids = inputs['input_ids'].to(device)\n",
    "                    attention_mask = inputs['attention_mask'].to(device)\n",
    "                    \n",
    "                    # Generate text\n",
    "                    with autocast():\n",
    "                        generated_ids = model.generate(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_length=min(max_length, input_ids.shape[1] + 150),\n",
    "                            temperature=temperature,\n",
    "                            do_sample=True,\n",
    "                            top_p=0.9,\n",
    "                            top_k=50,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            repetition_penalty=1.1,\n",
    "                            no_repeat_ngram_size=3\n",
    "                        )\n",
    "                    \n",
    "                    # Decode generated text\n",
    "                    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Remove the original prompt from generated text\n",
    "                    if generated_text.startswith(prompt):\n",
    "                        generated_text = prompt + generated_text[len(prompt):]\n",
    "                    \n",
    "                    print(f\"\\nSample {sample_idx + 1}:\")\n",
    "                    print(f\"ü§ñ {generated_text}\")\n",
    "                    print()\n",
    "                    \n",
    "                    # Store generation for analysis\n",
    "                    all_generations.append({\n",
    "                        'prompt': prompt,\n",
    "                        'generated_text': generated_text,\n",
    "                        'sample_idx': sample_idx + 1\n",
    "                    })\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during text generation: {e}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(all_generations)} text samples successfully!\")\n",
    "    return all_generations\n",
    "\n",
    "def analyze_generation_quality(generations, tokenizer):\n",
    "    \"\"\"Analyze the quality of generated text\"\"\"\n",
    "    \n",
    "    print(\"\\nüîç Generation Quality Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not generations:\n",
    "        print(\"‚ùå No generations to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_gens = len(generations)\n",
    "    avg_length = np.mean([len(gen['generated_text']) for gen in generations])\n",
    "    avg_tokens = np.mean([len(tokenizer.encode(gen['generated_text'])) for gen in generations])\n",
    "    \n",
    "    print(f\"üìä Basic Statistics:\")\n",
    "    print(f\"   Total generations: {total_gens}\")\n",
    "    print(f\"   Average character length: {avg_length:.1f}\")\n",
    "    print(f\"   Average token length: {avg_tokens:.1f}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    print(f\"\\nüîç Quality Checks:\")\n",
    "    \n",
    "    # Repetition check\n",
    "    repetitive_count = 0\n",
    "    for gen in generations:\n",
    "        text = gen['generated_text']\n",
    "        words = text.split()\n",
    "        if len(words) > 10:\n",
    "            # Check for repeated 3-grams\n",
    "            trigrams = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n",
    "            unique_trigrams = set(trigrams)\n",
    "            if len(unique_trigrams) / len(trigrams) < 0.8:  # Less than 80% unique trigrams\n",
    "                repetitive_count += 1\n",
    "    \n",
    "    print(f\"   Repetitive generations: {repetitive_count}/{total_gens} ({repetitive_count/total_gens*100:.1f}%)\")\n",
    "    \n",
    "    # Length variation\n",
    "    lengths = [len(gen['generated_text']) for gen in generations]\n",
    "    length_std = np.std(lengths)\n",
    "    print(f\"   Length variation (std): {length_std:.1f}\")\n",
    "    \n",
    "    # Check for coherence (basic)\n",
    "    coherent_count = 0\n",
    "    for gen in generations:\n",
    "        text = gen['generated_text']\n",
    "        # Basic coherence: contains periods, reasonable length, not all caps\n",
    "        if ('.' in text and len(text) > 50 and \n",
    "            not text.isupper() and not text.islower()):\n",
    "            coherent_count += 1\n",
    "    \n",
    "    print(f\"   Potentially coherent: {coherent_count}/{total_gens} ({coherent_count/total_gens*100:.1f}%)\")\n",
    "    \n",
    "    # Check vocabulary diversity\n",
    "    all_words = []\n",
    "    for gen in generations:\n",
    "        words = gen['generated_text'].lower().split()\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    unique_words = len(set(all_words))\n",
    "    total_words = len(all_words)\n",
    "    vocabulary_diversity = unique_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    print(f\"   Vocabulary diversity: {vocabulary_diversity:.3f}\")\n",
    "    print(f\"   Unique words: {unique_words:,}\")\n",
    "    print(f\"   Total words: {total_words:,}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    print(f\"\\nüèÜ Overall Assessment:\")\n",
    "    \n",
    "    quality_score = 0\n",
    "    if repetitive_count / total_gens < 0.3:  # Less than 30% repetitive\n",
    "        quality_score += 25\n",
    "    if coherent_count / total_gens > 0.7:  # More than 70% coherent\n",
    "        quality_score += 25\n",
    "    if vocabulary_diversity > 0.5:  # Good vocabulary diversity\n",
    "        quality_score += 25\n",
    "    if length_std > 20:  # Good length variation\n",
    "        quality_score += 25\n",
    "    \n",
    "    if quality_score >= 75:\n",
    "        assessment = \"üü¢ Excellent - Model generating high-quality, diverse text\"\n",
    "    elif quality_score >= 50:\n",
    "        assessment = \"üü° Good - Model showing decent performance\"\n",
    "    elif quality_score >= 25:\n",
    "        assessment = \"üü† Fair - Model needs more training\"\n",
    "    else:\n",
    "        assessment = \"üî¥ Poor - Model needs significant improvement\"\n",
    "    \n",
    "    print(f\"   Quality Score: {quality_score}/100\")\n",
    "    print(f\"   Assessment: {assessment}\")\n",
    "\n",
    "def interactive_generation(model, tokenizer):\n",
    "    \"\"\"Interactive text generation for manual testing\"\"\"\n",
    "    \n",
    "    print(\"\\nüéÆ Interactive Generation Mode\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Enter prompts to test the model (type 'quit' to exit)\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"\\nüéØ Enter prompt: \").strip()\n",
    "            \n",
    "            if prompt.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Exiting interactive mode\")\n",
    "                break\n",
    "                \n",
    "            if not prompt:\n",
    "                print(\"Please enter a valid prompt\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nü§ñ Generating response for: '{prompt}'\")\n",
    "            \n",
    "            # Generate single response\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with autocast():\n",
    "                    generated_ids = model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=min(200, input_ids.shape[1] + 100),\n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        top_p=0.9,\n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "            \n",
    "            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            print(f\"üìù Generated: {generated_text}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Exiting interactive mode\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Run text generation if model is available\n",
    "if 'model' in globals() and model is not None:\n",
    "    print(\"ü§ñ Model found! Running text generation tests...\")\n",
    "    \n",
    "    # Generate sample texts\n",
    "    sample_prompts = [\n",
    "        \"The future of artificial intelligence\",\n",
    "        \"In a world where technology\",\n",
    "        \"Scientists have recently discovered\",\n",
    "        \"The most important lesson in life is\",\n",
    "        \"Climate change represents\"\n",
    "    ]\n",
    "    \n",
    "    generations = generate_text_samples(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        prompts=sample_prompts,\n",
    "        max_length=150,\n",
    "        num_samples=2,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    if generations:\n",
    "        # Analyze generation quality\n",
    "        analyze_generation_quality(generations, tokenizer)\n",
    "        \n",
    "        # Save generations to file\n",
    "        try:\n",
    "            import json\n",
    "            with open('generated_samples.json', 'w') as f:\n",
    "                json.dump(generations, f, indent=2)\n",
    "            print(\"\\nüíæ Generated samples saved to 'generated_samples.json'\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving generations: {e}\")\n",
    "        \n",
    "        # Offer interactive mode\n",
    "        print(\"\\nüéÆ Want to try interactive generation? (y/n)\")\n",
    "        response = input().strip().lower()\n",
    "        if response == 'y':\n",
    "            interactive_generation(model, tokenizer)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Model not available. Please run training first (Cells 1-7)\")\n",
    "    print(\"üîß Showing what text generation output would look like...\")\n",
    "    \n",
    "    # Demo output format\n",
    "    print(\"\\nüìù Example generation output:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üéØ Prompt: 'The future of artificial intelligence'\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"ü§ñ Sample 1: The future of artificial intelligence will be shaped by...\")\n",
    "    print(\"ü§ñ Sample 2: The future of artificial intelligence holds great promise...\")\n",
    "    print(\"\\nüîç Quality analysis would appear here after real training.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: Save Final Model, Tokenizer, and Artifacts\n",
    "# Save the trained model, tokenizer files, and config to disk\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def save_final_model_and_artifacts(model, tokenizer, model_config, training_stats, save_dir=\"./smollm2_trained\"):\n",
    "    \"\"\"Save the trained model and all related artifacts\"\"\"\n",
    "    \n",
    "    print(\"üíæ Saving Trained Model and Artifacts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"‚ùå No model to save. Please run training first.\")\n",
    "        return False\n",
    "    \n",
    "    # Create save directory\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    print(f\"üìÅ Save directory: {save_path.absolute()}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Save the model\n",
    "        print(\"\\nü§ñ Saving model...\")\n",
    "        model_save_path = save_path / \"model\"\n",
    "        \n",
    "        # If model is wrapped with DataParallel, get the underlying model\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        \n",
    "        # Save model using transformers save_pretrained method\n",
    "        model_to_save.save_pretrained(model_save_path)\n",
    "        print(f\"‚úÖ Model saved to: {model_save_path}\")\n",
    "        \n",
    "        # 2. Save the tokenizer\n",
    "        print(\"\\nüî§ Saving tokenizer...\")\n",
    "        tokenizer_save_path = save_path / \"tokenizer\"\n",
    "        tokenizer.save_pretrained(tokenizer_save_path)\n",
    "        print(f\"‚úÖ Tokenizer saved to: {tokenizer_save_path}\")\n",
    "        \n",
    "        # 3. Save model configuration\n",
    "        print(\"\\n‚öôÔ∏è  Saving model configuration...\")\n",
    "        config_save_path = save_path / \"config.json\"\n",
    "        if model_config:\n",
    "            model_config.save_pretrained(save_path)\n",
    "            print(f\"‚úÖ Config saved to: {config_save_path}\")\n",
    "        \n",
    "        # 4. Save training statistics\n",
    "        print(\"\\nüìä Saving training statistics...\")\n",
    "        stats_save_path = save_path / \"training_stats.json\"\n",
    "        if training_stats and training_stats.get('steps'):\n",
    "            import json\n",
    "            with open(stats_save_path, 'w') as f:\n",
    "                json.dump(training_stats, f, indent=2)\n",
    "            print(f\"‚úÖ Training stats saved to: {stats_save_path}\")\n",
    "        \n",
    "        # 5. Save training metadata\n",
    "        print(\"\\nüìã Saving training metadata...\")\n",
    "        metadata = {\n",
    "            \"model_name\": \"SmolLM2-1.7B-Custom\",\n",
    "            \"training_dataset\": \"HuggingFaceTB/cosmopedia-100k\",\n",
    "            \"training_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"total_epochs\": 3,\n",
    "            \"effective_batch_size\": EFFECTIVE_BATCH_SIZE if 'EFFECTIVE_BATCH_SIZE' in globals() else \"Unknown\",\n",
    "            \"max_sequence_length\": MAX_SEQ_LENGTH if 'MAX_SEQ_LENGTH' in globals() else 1024,\n",
    "            \"learning_rate\": 5e-5,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"scheduler\": \"cosine_with_warmup\",\n",
    "            \"mixed_precision\": \"FP16\",\n",
    "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS if 'GRADIENT_ACCUMULATION_STEPS' in globals() else 8,\n",
    "            \"flash_attention\": \"Enabled\" if hasattr(model_config, 'use_flash_attention_2') and model_config.use_flash_attention_2 else \"Disabled\",\n",
    "            \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "            \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        }\n",
    "        \n",
    "        # Add final training statistics if available\n",
    "        if training_stats and training_stats.get('losses'):\n",
    "            metadata.update({\n",
    "                \"final_loss\": training_stats['losses'][-1],\n",
    "                \"initial_loss\": training_stats['losses'][0],\n",
    "                \"total_training_steps\": training_stats['steps'][-1] if training_stats['steps'] else 0,\n",
    "                \"loss_reduction_percent\": ((training_stats['losses'][0] - training_stats['losses'][-1]) / training_stats['losses'][0] * 100) if training_stats['losses'][0] > 0 else 0\n",
    "            })\n",
    "        \n",
    "        metadata_save_path = save_path / \"training_metadata.json\"\n",
    "        with open(metadata_save_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"‚úÖ Training metadata saved to: {metadata_save_path}\")\n",
    "        \n",
    "        # 6. Save model card (README)\n",
    "        print(\"\\nüìÑ Creating model card...\")\n",
    "        model_card_content = f\"\"\"# SmolLM2-1.7B Custom Trained Model\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This is a custom-trained version of SmolLM2-1.7B, trained from scratch on the Cosmopedia-100k dataset.\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base Architecture**: SmolLM2-1.7B\n",
    "- **Training Dataset**: HuggingFaceTB/cosmopedia-100k\n",
    "- **Training Date**: {metadata['training_date']}\n",
    "- **Total Parameters**: {metadata['total_parameters']:,}\n",
    "- **Trainable Parameters**: {metadata['trainable_parameters']:,}\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "- **Epochs**: {metadata['total_epochs']}\n",
    "- **Effective Batch Size**: {metadata['effective_batch_size']}\n",
    "- **Learning Rate**: {metadata['learning_rate']}\n",
    "- **Optimizer**: {metadata['optimizer']}\n",
    "- **Scheduler**: {metadata['scheduler']}\n",
    "- **Mixed Precision**: {metadata['mixed_precision']}\n",
    "- **Max Sequence Length**: {metadata['max_sequence_length']}\n",
    "- **Flash Attention**: {metadata['flash_attention']}\n",
    "\n",
    "## Training Results\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        if training_stats and training_stats.get('losses'):\n",
    "            model_card_content += f\"\"\"- **Initial Loss**: {metadata.get('initial_loss', 'N/A'):.4f}\n",
    "- **Final Loss**: {metadata.get('final_loss', 'N/A'):.4f}\n",
    "- **Loss Reduction**: {metadata.get('loss_reduction_percent', 'N/A'):.1f}%\n",
    "- **Total Training Steps**: {metadata.get('total_training_steps', 'N/A'):,}\n",
    "\"\"\"\n",
    "        \n",
    "        model_card_content += f\"\"\"\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{save_path}/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{save_path}/model\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100, do_sample=True, temperature=0.7)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "```\n",
    "\n",
    "## Training Infrastructure\n",
    "\n",
    "- **Framework**: PyTorch with Transformers\n",
    "- **GPU**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
    "- **CUDA Version**: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\n",
    "\n",
    "## Files in this Model\n",
    "\n",
    "- `model/`: Contains the trained model weights and configuration\n",
    "- `tokenizer/`: Contains the tokenizer files\n",
    "- `training_stats.json`: Detailed training statistics and loss curves\n",
    "- `training_metadata.json`: Training configuration and metadata\n",
    "- `README.md`: This file\n",
    "\n",
    "## Notes\n",
    "\n",
    "This model was trained from scratch (random initialization) rather than fine-tuning from pre-trained weights.\n",
    "\"\"\"\n",
    "        \n",
    "        readme_save_path = save_path / \"README.md\"\n",
    "        with open(readme_save_path, 'w') as f:\n",
    "            f.write(model_card_content)\n",
    "        print(f\"‚úÖ Model card saved to: {readme_save_path}\")\n",
    "        \n",
    "        # 7. Copy training plots if they exist\n",
    "        plot_files = ['training_metrics.png']\n",
    "        for plot_file in plot_files:\n",
    "            if Path(plot_file).exists():\n",
    "                shutil.copy(plot_file, save_path / plot_file)\n",
    "                print(f\"‚úÖ Copied {plot_file} to save directory\")\n",
    "        \n",
    "        # 8. Calculate total size\n",
    "        total_size = sum(f.stat().st_size for f in save_path.rglob('*') if f.is_file())\n",
    "        size_gb = total_size / (1024**3)\n",
    "        \n",
    "        print(f\"\\nüìä Save Summary:\")\n",
    "        print(f\"   Save directory: {save_path.absolute()}\")\n",
    "        print(f\"   Total size: {size_gb:.2f} GB\")\n",
    "        print(f\"   Files saved: {len(list(save_path.rglob('*')))}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Model and artifacts saved successfully!\")\n",
    "        print(f\"üéØ To use this model later:\")\n",
    "        print(f\"   from transformers import AutoTokenizer, AutoModelForCausalLM\")\n",
    "        print(f\"   tokenizer = AutoTokenizer.from_pretrained('{save_path}/tokenizer')\")\n",
    "        print(f\"   model = AutoModelForCausalLM.from_pretrained('{save_path}/model')\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving model: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_kaggle_dataset_metadata(save_dir=\"./smollm2_trained\"):\n",
    "    \"\"\"Create metadata for uploading to Kaggle Datasets\"\"\"\n",
    "    \n",
    "    print(\"\\nüì¶ Creating Kaggle Dataset Metadata\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    save_path = Path(save_dir)\n",
    "    if not save_path.exists():\n",
    "        print(\"‚ùå Save directory doesn't exist\")\n",
    "        return\n",
    "    \n",
    "    # Kaggle dataset metadata\n",
    "    kaggle_metadata = {\n",
    "        \"title\": \"SmolLM2-1.7B Trained on Cosmopedia-100k\",\n",
    "        \"id\": \"your-username/smollm2-cosmopedia-trained\",  # Update with your Kaggle username\n",
    "        \"licenses\": [{\"name\": \"apache-2.0\"}],\n",
    "        \"keywords\": [\"nlp\", \"language-model\", \"transformer\", \"pytorch\", \"smollm\"],\n",
    "        \"collaborators\": [],\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    kaggle_metadata_path = save_path / \"dataset-metadata.json\"\n",
    "    with open(kaggle_metadata_path, 'w') as f:\n",
    "        json.dump(kaggle_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Kaggle metadata saved to: {kaggle_metadata_path}\")\n",
    "    print(\"üìù Update the 'id' field with your Kaggle username before uploading\")\n",
    "\n",
    "# Execute saving if model is available\n",
    "if 'model' in globals() and model is not None:\n",
    "    print(\"ü§ñ Model found! Proceeding with save...\")\n",
    "    \n",
    "    # Get training stats if available\n",
    "    stats_to_save = training_stats if 'training_stats' in globals() else {}\n",
    "    config_to_save = model_config if 'model_config' in globals() else None\n",
    "    \n",
    "    # Save everything\n",
    "    success = save_final_model_and_artifacts(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        model_config=config_to_save,\n",
    "        training_stats=stats_to_save,\n",
    "        save_dir=\"./smollm2_trained\"\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        # Create Kaggle metadata\n",
    "        create_kaggle_dataset_metadata(\"./smollm2_trained\")\n",
    "        \n",
    "        print(\"\\nüéâ All artifacts saved successfully!\")\n",
    "        print(\"\\nüìã Next Steps:\")\n",
    "        print(\"1. Test the saved model by loading it back\")\n",
    "        print(\"2. Upload to Kaggle Datasets for sharing\")\n",
    "        print(\"3. Transfer to EC2 for continued training\")\n",
    "        print(\"4. Evaluate on additional benchmarks\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No trained model found to save.\")\n",
    "    print(\"Please run the training loop (Cell 7) first.\")\n",
    "    \n",
    "    # Show what would be saved\n",
    "    print(\"\\nüîß Files that would be saved:\")\n",
    "    print(\"üìÅ smollm2_trained/\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ model/\")\n",
    "    print(\"  ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin\")\n",
    "    print(\"  ‚îÇ   ‚îî‚îÄ‚îÄ config.json\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ tokenizer/\")\n",
    "    print(\"  ‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json\")\n",
    "    print(\"  ‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json\")\n",
    "    print(\"  ‚îÇ   ‚îî‚îÄ‚îÄ special_tokens_map.json\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ training_stats.json\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ training_metadata.json\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ training_metrics.png\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ README.md\")\n",
    "    print(\"  ‚îî‚îÄ‚îÄ dataset-metadata.json (for Kaggle)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: Visualization and Results - Final Analysis\n",
    "# Plot training loss curves, display training statistics, and comprehensive results summary\n",
    "\n",
    "def create_comprehensive_training_report(training_stats, model, tokenizer):\n",
    "    \"\"\"Create a comprehensive final training report with all visualizations\"\"\"\n",
    "    \n",
    "    print(\"üìä COMPREHENSIVE TRAINING REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üéØ SmolLM2-1.7B Training from Scratch on Cosmopedia-100k\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if not training_stats or not training_stats.get('steps'):\n",
    "        print(\"‚ùå No training data available for comprehensive report\")\n",
    "        return\n",
    "    \n",
    "    # 1. Training Overview\n",
    "    print(\"\\nüîç TRAINING OVERVIEW\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_steps = training_stats['steps'][-1] if training_stats['steps'] else 0\n",
    "    total_logged_points = len(training_stats['steps'])\n",
    "    training_duration = \"~3 epochs\"  # Based on our setup\n",
    "    \n",
    "    print(f\"üìà Training Completed Successfully!\")\n",
    "    print(f\"   Total Training Steps: {total_steps:,}\")\n",
    "    print(f\"   Logged Data Points: {total_logged_points:,}\")\n",
    "    print(f\"   Training Duration: {training_duration}\")\n",
    "    print(f\"   Dataset: Cosmopedia-100k (100,000 samples)\")\n",
    "    print(f\"   Model: SmolLM2-1.7B (trained from scratch)\")\n",
    "    \n",
    "    # 2. Loss Analysis\n",
    "    print(f\"\\nüìâ LOSS ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    initial_loss = training_stats['losses'][0]\n",
    "    final_loss = training_stats['losses'][-1]\n",
    "    min_loss = min(training_stats['losses'])\n",
    "    max_loss = max(training_stats['losses'])\n",
    "    avg_loss = np.mean(training_stats['losses'])\n",
    "    \n",
    "    loss_reduction = ((initial_loss - final_loss) / initial_loss * 100) if initial_loss > 0 else 0\n",
    "    convergence_ratio = min_loss / initial_loss if initial_loss > 0 else 0\n",
    "    \n",
    "    print(f\"üìä Loss Statistics:\")\n",
    "    print(f\"   Initial Loss: {initial_loss:.4f}\")\n",
    "    print(f\"   Final Loss: {final_loss:.4f}\")\n",
    "    print(f\"   Minimum Loss: {min_loss:.4f}\")\n",
    "    print(f\"   Maximum Loss: {max_loss:.4f}\")\n",
    "    print(f\"   Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"   Loss Reduction: {loss_reduction:.2f}%\")\n",
    "    print(f\"   Convergence Ratio: {convergence_ratio:.4f}\")\n",
    "    \n",
    "    # Loss improvement assessment\n",
    "    if loss_reduction > 50:\n",
    "        loss_assessment = \"üü¢ Excellent - Strong learning progress\"\n",
    "    elif loss_reduction > 30:\n",
    "        loss_assessment = \"üü° Good - Solid improvement\"\n",
    "    elif loss_reduction > 10:\n",
    "        loss_assessment = \"üü† Fair - Some improvement\"\n",
    "    else:\n",
    "        loss_assessment = \"üî¥ Poor - Limited learning\"\n",
    "    \n",
    "    print(f\"   Assessment: {loss_assessment}\")\n",
    "    \n",
    "    # 3. Training Stability Analysis\n",
    "    print(f\"\\n‚öñÔ∏è  TRAINING STABILITY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate volatility (standard deviation of losses)\n",
    "    loss_volatility = np.std(training_stats['losses'])\n",
    "    \n",
    "    # Trend analysis on recent losses\n",
    "    if len(training_stats['losses']) >= 20:\n",
    "        recent_losses = training_stats['losses'][-20:]\n",
    "        trend_slope = np.polyfit(range(len(recent_losses)), recent_losses, 1)[0]\n",
    "        \n",
    "        if abs(trend_slope) < 0.001:\n",
    "            trend_status = \"üü¢ Converging - Loss stabilizing\"\n",
    "        elif trend_slope < -0.01:\n",
    "            trend_status = \"üü° Still Learning - Loss decreasing rapidly\"\n",
    "        else:\n",
    "            trend_status = \"üî¥ Potentially Diverging - Loss increasing\"\n",
    "    else:\n",
    "        trend_status = \"üìä Insufficient data for trend analysis\"\n",
    "        trend_slope = 0\n",
    "    \n",
    "    print(f\"üìà Stability Metrics:\")\n",
    "    print(f\"   Loss Volatility (œÉ): {loss_volatility:.4f}\")\n",
    "    print(f\"   Recent Trend Slope: {trend_slope:.6f}\")\n",
    "    print(f\"   Trend Status: {trend_status}\")\n",
    "    \n",
    "    # 4. Learning Rate Analysis\n",
    "    if training_stats.get('learning_rates'):\n",
    "        print(f\"\\nüìà LEARNING RATE SCHEDULE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        initial_lr = training_stats['learning_rates'][0]\n",
    "        final_lr = training_stats['learning_rates'][-1]\n",
    "        max_lr = max(training_stats['learning_rates'])\n",
    "        \n",
    "        print(f\"üìä Learning Rate Statistics:\")\n",
    "        print(f\"   Initial LR: {initial_lr:.2e}\")\n",
    "        print(f\"   Final LR: {final_lr:.2e}\")\n",
    "        print(f\"   Maximum LR: {max_lr:.2e}\")\n",
    "        print(f\"   LR Decay Ratio: {final_lr/initial_lr:.4f}\")\n",
    "    \n",
    "    # 5. Resource Utilization\n",
    "    if training_stats.get('gpu_memory') and any(mem > 0 for mem in training_stats['gpu_memory']):\n",
    "        print(f\"\\nüíæ RESOURCE UTILIZATION\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        avg_memory = np.mean(training_stats['gpu_memory'])\n",
    "        max_memory = max(training_stats['gpu_memory'])\n",
    "        min_memory = min(training_stats['gpu_memory'])\n",
    "        memory_efficiency = avg_memory / max_memory if max_memory > 0 else 0\n",
    "        \n",
    "        print(f\"üìä GPU Memory Statistics:\")\n",
    "        print(f\"   Average Usage: {avg_memory:.2f} GB\")\n",
    "        print(f\"   Peak Usage: {max_memory:.2f} GB\")\n",
    "        print(f\"   Minimum Usage: {min_memory:.2f} GB\")\n",
    "        print(f\"   Memory Efficiency: {memory_efficiency:.2%}\")\n",
    "    \n",
    "    # 6. Model Performance Summary\n",
    "    if model is not None:\n",
    "        print(f\"\\nü§ñ MODEL PERFORMANCE SUMMARY\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"üìä Model Statistics:\")\n",
    "        print(f\"   Total Parameters: {total_params/1e9:.2f}B ({total_params:,})\")\n",
    "        print(f\"   Trainable Parameters: {trainable_params/1e9:.2f}B ({trainable_params:,})\")\n",
    "        print(f\"   Parameter Efficiency: {trainable_params/total_params:.2%}\")\n",
    "        \n",
    "        # Calculate parameters per loss improvement\n",
    "        if loss_reduction > 0:\n",
    "            param_efficiency = total_params / loss_reduction\n",
    "            print(f\"   Parameters per % Loss Reduction: {param_efficiency/1e6:.1f}M\")\n",
    "\n",
    "def create_final_visualization_suite(training_stats):\n",
    "    \"\"\"Create comprehensive visualization suite for final analysis\"\"\"\n",
    "    \n",
    "    print(\"\\nüé® Creating Final Visualization Suite\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not training_stats or not training_stats.get('steps'):\n",
    "        print(\"‚ùå No training data for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create a large comprehensive plot\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Main title\n",
    "    fig.suptitle('SmolLM2-1.7B Training Analysis - Comprehensive Report', \n",
    "                 fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # 1. Training Loss (Large plot)\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    ax1.plot(training_stats['steps'], training_stats['losses'], 'b-', linewidth=2, alpha=0.8, label='Training Loss')\n",
    "    ax1.set_xlabel('Training Step', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(training_stats['steps']) > 10:\n",
    "        z = np.polyfit(training_stats['steps'], np.log(training_stats['losses']), 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax1.plot(training_stats['steps'], np.exp(p(training_stats['steps'])), \n",
    "                'r--', alpha=0.7, linewidth=2, label='Trend Line')\n",
    "    \n",
    "    # Add annotations for key points\n",
    "    min_loss_idx = np.argmin(training_stats['losses'])\n",
    "    ax1.annotate(f'Min Loss: {min(training_stats[\"losses\"]):.4f}', \n",
    "                xy=(training_stats['steps'][min_loss_idx], training_stats['losses'][min_loss_idx]),\n",
    "                xytext=(10, 20), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    ax1.legend(fontsize=12)\n",
    "    \n",
    "    # 2. Loss Distribution\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    ax2.hist(training_stats['losses'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.set_xlabel('Loss Value', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Loss Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Learning Rate Schedule\n",
    "    if training_stats.get('learning_rates'):\n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "        ax3.plot(training_stats['steps'], training_stats['learning_rates'], 'g-', linewidth=2)\n",
    "        ax3.set_xlabel('Training Step', fontsize=12)\n",
    "        ax3.set_ylabel('Learning Rate', fontsize=12)\n",
    "        ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_yscale('log')\n",
    "    \n",
    "    # 4. Loss Smoothed (Moving Average)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    window_size = max(1, len(training_stats['losses']) // 20)  # 5% of data points\n",
    "    if len(training_stats['losses']) >= window_size:\n",
    "        smoothed_losses = np.convolve(training_stats['losses'], \n",
    "                                    np.ones(window_size)/window_size, mode='valid')\n",
    "        smoothed_steps = training_stats['steps'][window_size-1:]\n",
    "        ax4.plot(smoothed_steps, smoothed_losses, 'purple', linewidth=2)\n",
    "    ax4.set_xlabel('Training Step', fontsize=12)\n",
    "    ax4.set_ylabel('Smoothed Loss', fontsize=12)\n",
    "    ax4.set_title(f'Smoothed Loss (Window: {window_size})', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. GPU Memory Usage\n",
    "    if training_stats.get('gpu_memory') and any(mem > 0 for mem in training_stats['gpu_memory']):\n",
    "        ax5 = fig.add_subplot(gs[1, 2])\n",
    "        ax5.plot(training_stats['steps'], training_stats['gpu_memory'], 'r-', linewidth=2)\n",
    "        ax5.set_xlabel('Training Step', fontsize=12)\n",
    "        ax5.set_ylabel('GPU Memory (GB)', fontsize=12)\n",
    "        ax5.set_title('GPU Memory Usage', fontsize=14, fontweight='bold')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add average line\n",
    "        avg_memory = np.mean(training_stats['gpu_memory'])\n",
    "        ax5.axhline(y=avg_memory, color='orange', linestyle='--', \n",
    "                   label=f'Avg: {avg_memory:.1f}GB', linewidth=2)\n",
    "        ax5.legend()\n",
    "    \n",
    "    # 6. Loss vs Learning Rate (if both available)\n",
    "    if training_stats.get('learning_rates') and len(training_stats['learning_rates']) == len(training_stats['losses']):\n",
    "        ax6 = fig.add_subplot(gs[2, 0])\n",
    "        scatter = ax6.scatter(training_stats['learning_rates'], training_stats['losses'], \n",
    "                            c=training_stats['steps'], cmap='viridis', alpha=0.6, s=20)\n",
    "        ax6.set_xlabel('Learning Rate', fontsize=12)\n",
    "        ax6.set_ylabel('Loss', fontsize=12)\n",
    "        ax6.set_title('Loss vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "        ax6.set_xscale('log')\n",
    "        ax6.set_yscale('log')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax6, label='Training Step')\n",
    "    \n",
    "    # 7. Training Progress per Epoch\n",
    "    if training_stats.get('epochs'):\n",
    "        ax7 = fig.add_subplot(gs[2, 1])\n",
    "        epochs = training_stats['epochs']\n",
    "        unique_epochs = sorted(set(epochs))\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for epoch in unique_epochs:\n",
    "            epoch_indices = [i for i, e in enumerate(epochs) if e == epoch]\n",
    "            if epoch_indices:\n",
    "                epoch_loss = np.mean([training_stats['losses'][i] for i in epoch_indices])\n",
    "                epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        if epoch_losses:\n",
    "            bars = ax7.bar(unique_epochs, epoch_losses, alpha=0.7, \n",
    "                          color=['#FF6B6B', '#4ECDC4', '#45B7D1'][:len(unique_epochs)])\n",
    "            ax7.set_xlabel('Epoch', fontsize=12)\n",
    "            ax7.set_ylabel('Average Loss', fontsize=12)\n",
    "            ax7.set_title('Average Loss per Epoch', fontsize=14, fontweight='bold')\n",
    "            ax7.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for i, (bar, val) in enumerate(zip(bars, epoch_losses)):\n",
    "                ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                        f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 8. Loss Improvement Rate\n",
    "    ax8 = fig.add_subplot(gs[2, 2])\n",
    "    if len(training_stats['losses']) > 1:\n",
    "        loss_diffs = np.diff(training_stats['losses'])\n",
    "        ax8.plot(training_stats['steps'][1:], loss_diffs, 'orange', linewidth=1, alpha=0.7)\n",
    "        ax8.axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "        ax8.set_xlabel('Training Step', fontsize=12)\n",
    "        ax8.set_ylabel('Loss Change per Step', fontsize=12)\n",
    "        ax8.set_title('Loss Improvement Rate', fontsize=14, fontweight='bold')\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Training Summary Text\n",
    "    ax9 = fig.add_subplot(gs[3, :])\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    summary_text = f\"\"\"\n",
    "TRAINING SUMMARY\n",
    "‚Ä¢ Dataset: Cosmopedia-100k (100,000 samples) ‚Ä¢ Model: SmolLM2-1.7B (trained from scratch)\n",
    "‚Ä¢ Total Steps: {training_stats['steps'][-1]:,} ‚Ä¢ Loss Reduction: {((training_stats['losses'][0] - training_stats['losses'][-1]) / training_stats['losses'][0] * 100):.1f}%\n",
    "‚Ä¢ Initial Loss: {training_stats['losses'][0]:.4f} ‚Ä¢ Final Loss: {training_stats['losses'][-1]:.4f} ‚Ä¢ Min Loss: {min(training_stats['losses']):.4f}\n",
    "‚Ä¢ Training Configuration: FP16 Mixed Precision, Flash Attention, Cosine LR Schedule, AdamW Optimizer\n",
    "‚Ä¢ Optimization: Gradient Accumulation, Gradient Clipping, Multi-GPU Support\n",
    "\"\"\"\n",
    "    \n",
    "    ax9.text(0.5, 0.5, summary_text, ha='center', va='center', \n",
    "            fontsize=14, bbox=dict(boxstyle='round,pad=1', facecolor='lightblue', alpha=0.8),\n",
    "            transform=ax9.transAxes, fontweight='bold')\n",
    "    \n",
    "    # Save the comprehensive plot\n",
    "    plt.savefig('comprehensive_training_analysis.png', dpi=300, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    print(\"‚úÖ Comprehensive visualization saved as 'comprehensive_training_analysis.png'\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_final_metrics_summary(training_stats):\n",
    "    \"\"\"Generate final numerical metrics summary\"\"\"\n",
    "    \n",
    "    print(\"\\nüìã FINAL METRICS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not training_stats or not training_stats.get('steps'):\n",
    "        print(\"‚ùå No training data available\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculate all key metrics\n",
    "    metrics = {\n",
    "        'total_steps': training_stats['steps'][-1],\n",
    "        'initial_loss': training_stats['losses'][0],\n",
    "        'final_loss': training_stats['losses'][-1],\n",
    "        'min_loss': min(training_stats['losses']),\n",
    "        'max_loss': max(training_stats['losses']),\n",
    "        'avg_loss': np.mean(training_stats['losses']),\n",
    "        'loss_std': np.std(training_stats['losses']),\n",
    "        'loss_reduction_percent': ((training_stats['losses'][0] - training_stats['losses'][-1]) / training_stats['losses'][0] * 100) if training_stats['losses'][0] > 0 else 0,\n",
    "        'convergence_ratio': min(training_stats['losses']) / training_stats['losses'][0] if training_stats['losses'][0] > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    # Add learning rate metrics if available\n",
    "    if training_stats.get('learning_rates'):\n",
    "        metrics.update({\n",
    "            'initial_lr': training_stats['learning_rates'][0],\n",
    "            'final_lr': training_stats['learning_rates'][-1],\n",
    "            'max_lr': max(training_stats['learning_rates']),\n",
    "            'lr_decay_ratio': training_stats['learning_rates'][-1] / training_stats['learning_rates'][0]\n",
    "        })\n",
    "    \n",
    "    # Add GPU memory metrics if available\n",
    "    if training_stats.get('gpu_memory') and any(mem > 0 for mem in training_stats['gpu_memory']):\n",
    "        metrics.update({\n",
    "            'avg_gpu_memory': np.mean(training_stats['gpu_memory']),\n",
    "            'peak_gpu_memory': max(training_stats['gpu_memory']),\n",
    "            'min_gpu_memory': min(training_stats['gpu_memory'])\n",
    "        })\n",
    "    \n",
    "    # Print formatted metrics\n",
    "    print(\"üî¢ Key Performance Indicators:\")\n",
    "    print(f\"   Loss Reduction: {metrics['loss_reduction_percent']:.2f}%\")\n",
    "    print(f\"   Convergence Ratio: {metrics['convergence_ratio']:.4f}\")\n",
    "    print(f\"   Training Stability (œÉ): {metrics['loss_std']:.4f}\")\n",
    "    print(f\"   Final Performance: {metrics['final_loss']:.4f}\")\n",
    "    \n",
    "    # Overall grade\n",
    "    score = 0\n",
    "    if metrics['loss_reduction_percent'] > 50: score += 40\n",
    "    elif metrics['loss_reduction_percent'] > 30: score += 30\n",
    "    elif metrics['loss_reduction_percent'] > 10: score += 20\n",
    "    \n",
    "    if metrics['convergence_ratio'] < 0.3: score += 30\n",
    "    elif metrics['convergence_ratio'] < 0.5: score += 20\n",
    "    elif metrics['convergence_ratio'] < 0.7: score += 10\n",
    "    \n",
    "    if metrics['loss_std'] < 0.5: score += 20\n",
    "    elif metrics['loss_std'] < 1.0: score += 15\n",
    "    elif metrics['loss_std'] < 2.0: score += 10\n",
    "    \n",
    "    if metrics['final_loss'] < 2.0: score += 10\n",
    "    elif metrics['final_loss'] < 3.0: score += 5\n",
    "    \n",
    "    if score >= 90:\n",
    "        grade = \"A+ üèÜ Exceptional Training\"\n",
    "    elif score >= 80:\n",
    "        grade = \"A ü•á Excellent Training\"\n",
    "    elif score >= 70:\n",
    "        grade = \"B+ ü•à Very Good Training\"\n",
    "    elif score >= 60:\n",
    "        grade = \"B ü•â Good Training\"\n",
    "    elif score >= 50:\n",
    "        grade = \"C+ ‚≠ê Acceptable Training\"\n",
    "    else:\n",
    "        grade = \"C ‚ö†Ô∏è Needs Improvement\"\n",
    "    \n",
    "    print(f\"\\nüèÜ OVERALL TRAINING GRADE: {grade}\")\n",
    "    print(f\"üìä Training Score: {score}/100\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Execute comprehensive analysis if training stats are available\n",
    "if 'training_stats' in globals() and training_stats and training_stats.get('steps'):\n",
    "    print(\"üéØ Training data found! Generating comprehensive analysis...\")\n",
    "    \n",
    "    # Get model and tokenizer if available\n",
    "    analysis_model = model if 'model' in globals() else None\n",
    "    analysis_tokenizer = tokenizer if 'tokenizer' in globals() else None\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    create_comprehensive_training_report(training_stats, analysis_model, analysis_tokenizer)\n",
    "    \n",
    "    # Create visualization suite\n",
    "    create_final_visualization_suite(training_stats)\n",
    "    \n",
    "    # Generate final metrics\n",
    "    final_metrics = generate_final_metrics_summary(training_stats)\n",
    "    \n",
    "    # Save final metrics\n",
    "    try:\n",
    "        import json\n",
    "        with open('final_training_metrics.json', 'w') as f:\n",
    "            json.dump(final_metrics, f, indent=2)\n",
    "        print(\"\\nüíæ Final metrics saved to 'final_training_metrics.json'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving final metrics: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "    print(\"üéØ SmolLM2-1.7B has been successfully trained from scratch\")\n",
    "    print(\"üìä All visualizations and metrics have been generated\")\n",
    "    print(\"üíæ Model and artifacts are ready for deployment\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No training statistics available for comprehensive analysis.\")\n",
    "    print(\"Please run the training loop (Cell 7) first to generate training data.\")\n",
    "    \n",
    "    # Show what the final analysis would look like\n",
    "    print(\"\\nüîß This cell would generate:\")\n",
    "    print(\"üìä Comprehensive training report with detailed statistics\")\n",
    "    print(\"üìà Advanced visualization suite with multiple plots\")\n",
    "    print(\"üèÜ Final performance grading and assessment\")\n",
    "    print(\"üíæ Complete metrics export for future reference\")\n",
    "    print(\"üéØ Professional training summary for presentations\")\n",
    "    \n",
    "    print(\"\\nüìù Example output:\")\n",
    "    print(\"üèÜ OVERALL TRAINING GRADE: A ü•á Excellent Training\")\n",
    "    print(\"üìä Training Score: 85/100\")\n",
    "    print(\"üî¢ Loss Reduction: 65.3%\")\n",
    "    print(\"‚öñÔ∏è Training Stability: Excellent\")\n",
    "    print(\"üéØ Model Performance: High Quality\") "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
