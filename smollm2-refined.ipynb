{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate bitsandbytes einops peft tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup and Imports\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Required packages\n",
    "required_packages = [\n",
    "    \"transformers>=4.36.0\",\n",
    "    \"datasets>=2.14.0\", \n",
    "    \"torch>=2.0.0\",\n",
    "    \"accelerate>=0.24.0\",\n",
    "    \"flash-attn>=2.3.0\",\n",
    "    \"wandb\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"numpy\",\n",
    "    \"tqdm\"\n",
    "]\n",
    "\n",
    "# Install packages if needed (uncomment if running first time)\n",
    "# for package in required_packages:\n",
    "#     install_package(package.split(\">=\")[0])\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Transformers and datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✅ Environment setup complete\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: GPU Check and Random Seed Setup\n",
    "def setup_device_and_seed(seed=42):\n",
    "    \"\"\"Setup device, log GPU info, and set random seeds\"\"\"\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        num_gpus = 0\n",
    "    \n",
    "    return device, num_gpus\n",
    "\n",
    "# Setup device and seeds\n",
    "SEED = 42\n",
    "device, num_gpus = setup_device_and_seed(SEED)\n",
    "\n",
    "# Training configuration\n",
    "MULTI_GPU = num_gpus > 1\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"✅ Device setup complete - Using {device} with {num_gpus} GPUs\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Load and Explore Dataset (Cosmopedia 100k)\n",
    "def load_and_explore_dataset():\n",
    "    \"\"\"Load Cosmopedia 100k dataset\"\"\"\n",
    "    \n",
    "    try:\n",
    "        ds = load_dataset(\"HuggingFaceTB/cosmopedia-100k\", split=\"train\")\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_and_explore_dataset()\n",
    "\n",
    "# Store dataset info for later use\n",
    "if dataset:\n",
    "    DATASET_SIZE = len(dataset)\n",
    "    print(f\"✅ Dataset loaded - {DATASET_SIZE:,} samples\")\n",
    "else:\n",
    "    print(\"❌ Failed to load dataset. Please check your connection and try again.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Tokenizer - Existing Tokenizer\n",
    "def load_smollm2_tokenizer():\n",
    "    \"\"\"Load the existing SmolLM2 tokenizer\"\"\"\n",
    "    \n",
    "    model_id = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        # Add padding token if it doesn't exist\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        return tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading tokenizer: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = load_smollm2_tokenizer()\n",
    "\n",
    "if tokenizer:\n",
    "    # Set sequence length for training\n",
    "    MAX_SEQ_LENGTH = 2048\n",
    "    print(\"✅ Tokenizer loaded successfully\")\n",
    "else:\n",
    "    print(\"❌ Failed to load tokenizer. Please check your connection and model ID.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Dataset Preprocessing - ULTRA OPTIMIZED (Minimal Memory)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CosmopediaDataset(Dataset):\n",
    "    \"\"\"Ultra Memory-Optimized Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer, max_length=2048):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get text from dataset\n",
    "        text = self.dataset[idx]['text']\n",
    "        \n",
    "        # Tokenize with aggressive truncation\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract input_ids and attention_mask\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # For causal language modeling, labels are the same as input_ids\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def preprocess_dataset_ultra_optimized(dataset, tokenizer, max_length=2048, batch_size=1):\n",
    "    \"\"\"Ultra memory-optimized preprocessing\"\"\"\n",
    "    \n",
    "    if dataset is None or tokenizer is None:\n",
    "        print(\"❌ Dataset or tokenizer is None. Please load them first.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Create custom dataset\n",
    "    torch_dataset = CosmopediaDataset(dataset, tokenizer, max_length)\n",
    "    \n",
    "    # Create DataLoader with ultra optimization\n",
    "    dataloader = DataLoader(\n",
    "        torch_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    return torch_dataset, dataloader\n",
    "\n",
    "# ULTRA OPTIMIZED Configuration\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 64\n",
    "MAX_SEQ_LENGTH_ULTRA = 2048\n",
    "\n",
    "# Calculate effective batch size\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * max(1, num_gpus)\n",
    "\n",
    "# Clear GPU memory aggressively\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Preprocess dataset with ultra settings\n",
    "train_dataset, train_dataloader = preprocess_dataset_ultra_optimized(\n",
    "    dataset, \n",
    "    tokenizer, \n",
    "    max_length=MAX_SEQ_LENGTH_ULTRA, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "if train_dataloader:\n",
    "    # Calculate final training statistics\n",
    "    STEPS_PER_EPOCH = len(train_dataloader)\n",
    "    TOTAL_STEPS = STEPS_PER_EPOCH * 3  # 3 epochs\n",
    "    \n",
    "    print(f\"✅ Dataset preprocessing complete - {len(train_dataloader):,} batches\")\n",
    "else:\n",
    "    print(\"❌ Failed to create DataLoader. Please check your setup.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Model Configuration and Initialization (NO MIXED PRECISION VERSION)\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def setup_model_and_training_components_no_amp():\n",
    "    \"\"\"Initialize SmolLM2 model with random weights and setup training components - NO MIXED PRECISION\"\"\"\n",
    "    \n",
    "    # Clear GPU cache before model initialization\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load SmolLM2 configuration\n",
    "    model_id = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "    \n",
    "    try:\n",
    "        # Load config only (not weights)\n",
    "        config = AutoConfig.from_pretrained(model_id)\n",
    "        \n",
    "        # MEMORY OPTIMIZATION: Enable gradient checkpointing\n",
    "        config.use_cache = False\n",
    "        config.gradient_checkpointing = True\n",
    "        \n",
    "        # Enable Flash Attention 2 if available\n",
    "        try:\n",
    "            config.use_flash_attention_2 = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading config: {e}\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Initialize model with random weights\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing model: {e}\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup optimizer (AdamW)\n",
    "    learning_rate = 3e-5 \n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    # No weight decay for bias and layer norm parameters\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.95),\n",
    "        eps=1e-8,\n",
    "        foreach=False\n",
    "    )\n",
    "    \n",
    "    # Setup learning rate scheduler\n",
    "    num_warmup_steps = int(0.02 * TOTAL_STEPS)  # 2% warmup\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=TOTAL_STEPS\n",
    "    )\n",
    "    \n",
    "    # NO MIXED PRECISION - Use None for scaler\n",
    "    scaler = None\n",
    "    \n",
    "    # Final memory check\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return model, optimizer, scheduler, scaler, config\n",
    "\n",
    "# Initialize model and training components WITHOUT mixed precision\n",
    "model, optimizer, scheduler, scaler, model_config = setup_model_and_training_components_no_amp()\n",
    "\n",
    "if model is not None:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"✅ Model initialized - {total_params/1e9:.2f}B parameters\")\n",
    "else:\n",
    "    print(\"❌ Failed to initialize model. Please check your setup.\")\n",
    "\n",
    "# Gradient clipping value\n",
    "MAX_GRAD_NORM = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7:  Training Loop - EC2 with Comprehensive Logging\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import signal\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "# Set environment variables for maximum stability\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Training tracking variables\n",
    "training_stats = {\n",
    "    'steps': [],\n",
    "    'losses': [],\n",
    "    'learning_rates': [],\n",
    "    'epochs': [],\n",
    "    'gpu_memory': []\n",
    "}\n",
    "\n",
    "# Global flag for graceful shutdown\n",
    "shutdown_requested = False\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle spot instance interruption signals\"\"\"\n",
    "    global shutdown_requested\n",
    "    print(\"Spot instance interruption detected. Saving checkpoint...\")\n",
    "    log_message(\"INTERRUPTION: Spot instance termination signal received\")\n",
    "    shutdown_requested = True\n",
    "\n",
    "# Register signal handlers for spot instance interruptions\n",
    "signal.signal(signal.SIGTERM, signal_handler)\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "def log_message(message, log_file=\"training_log.md\"):\n",
    "    \"\"\"Log messages to markdown file with timestamp\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"**{timestamp}** - {message}\\n\\n\")\n",
    "\n",
    "def initialize_log():\n",
    "    \"\"\"Initialize the training log file\"\"\"\n",
    "    log_file = \"training_log.md\"\n",
    "    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# SmolLM2 Training Log\\n\\n\")\n",
    "        f.write(f\"**Training Started:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(\"## Configuration\\n\")\n",
    "        f.write(f\"- Model: SmolLM2-1.7B\\n\")\n",
    "        f.write(f\"- Dataset: Cosmopedia-100k\\n\")\n",
    "        f.write(f\"- Batch Size: {BATCH_SIZE}\\n\")\n",
    "        f.write(f\"- Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\\n\")\n",
    "        f.write(f\"- Effective Batch Size: {EFFECTIVE_BATCH_SIZE}\\n\")\n",
    "        f.write(f\"- Max Sequence Length: {MAX_SEQ_LENGTH_ULTRA}\\n\")\n",
    "        f.write(f\"- Total Steps: {TOTAL_STEPS:,}\\n\")\n",
    "        f.write(f\"- Log Interval: 50 steps\\n\")\n",
    "        f.write(f\"- Checkpoint Interval: 200 steps\\n\\n\")\n",
    "        f.write(\"## Training Progress\\n\\n\")\n",
    "\n",
    "def log_training_step(step, total_steps, loss, lr, gpu_memory, epoch):\n",
    "    \"\"\"Log training step details\"\"\"\n",
    "    with open(\"training_log.md\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"| Step {step:,}/{total_steps:,} | Epoch {epoch} | Loss: {loss:.4f} | LR: {lr:.2e} | GPU: {gpu_memory:.1f}GB |\\n\")\n",
    "\n",
    "def log_checkpoint(step, loss):\n",
    "    \"\"\"Log checkpoint saves\"\"\"\n",
    "    log_message(f\"CHECKPOINT: Saved at step {step:,}, loss {loss:.4f}\")\n",
    "\n",
    "def log_epoch_summary(epoch, avg_loss, time_minutes, steps):\n",
    "    \"\"\"Log epoch completion summary\"\"\"\n",
    "    log_message(f\"EPOCH {epoch} COMPLETE: Avg Loss {avg_loss:.4f}, Steps {steps:,}, Time {time_minutes:.1f}m\")\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Ultra-aggressive memory cleanup\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "def safe_checkpoint_save(model, optimizer, scheduler, step, epoch, loss):\n",
    "    \"\"\"Production checkpoint saving - spot instance optimized\"\"\"\n",
    "    \n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'training_stats': training_stats\n",
    "    }\n",
    "    \n",
    "    # Save directly in working directory\n",
    "    checkpoint_path = f\"checkpoint_step_{step}.pt\"\n",
    "    temp_path = f\"temp_checkpoint_step_{step}.pt\"\n",
    "    \n",
    "    try:\n",
    "        torch.save(checkpoint, temp_path)\n",
    "        \n",
    "        # Verify checkpoint integrity\n",
    "        test_load = torch.load(temp_path, map_location='cpu')\n",
    "        if 'step' in test_load and test_load['step'] == step:\n",
    "            shutil.move(temp_path, checkpoint_path)\n",
    "            print(f\"Checkpoint saved: step {step}, loss {loss:.4f}\")\n",
    "            log_checkpoint(step, loss)\n",
    "        else:\n",
    "            raise Exception(\"Checkpoint verification failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "        log_message(f\"ERROR: Checkpoint save failed at step {step}: {e}\")\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        return None\n",
    "    \n",
    "    aggressive_memory_cleanup()\n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint_safe(checkpoint_path, model, optimizer, scheduler):\n",
    "    \"\"\"Safe checkpoint loading with verification\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        if 'model_state_dict' not in checkpoint:\n",
    "            raise Exception(\"Invalid checkpoint format\")\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        start_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['loss']\n",
    "        \n",
    "        global training_stats\n",
    "        training_stats = checkpoint.get('training_stats', training_stats)\n",
    "        \n",
    "        print(f\"Resumed from step {start_step}, loss {best_loss:.4f}\")\n",
    "        log_message(f\"RESUMED: From step {start_step:,}, loss {best_loss:.4f}\")\n",
    "        return start_step, start_epoch, best_loss\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        log_message(f\"ERROR: Checkpoint load failed: {e}\")\n",
    "        return 0, 0, float('inf')\n",
    "\n",
    "def find_latest_checkpoint():\n",
    "    \"\"\"Find the latest valid checkpoint\"\"\"\n",
    "    \n",
    "    checkpoints = glob.glob(\"checkpoint_step_*.pt\")\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "        \n",
    "    valid_checkpoints = []\n",
    "    for cp in checkpoints:\n",
    "        try:\n",
    "            test = torch.load(cp, map_location='cpu')\n",
    "            if 'step' in test:\n",
    "                valid_checkpoints.append(cp)\n",
    "        except:\n",
    "            print(f\"Removing corrupted checkpoint: {cp}\")\n",
    "            os.remove(cp)\n",
    "    \n",
    "    if not valid_checkpoints:\n",
    "        return None\n",
    "        \n",
    "    valid_checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    return valid_checkpoints[-1]\n",
    "\n",
    "def cleanup_old_checkpoints():\n",
    "    \"\"\"Keep checkpoints more conservatively - every 2000 steps + last 5\"\"\"\n",
    "    checkpoints = glob.glob(\"checkpoint_step_*.pt\")\n",
    "    if len(checkpoints) <= 5:\n",
    "        return\n",
    "        \n",
    "    checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    \n",
    "    # Keep last 5 checkpoints always\n",
    "    keep_checkpoints = set(checkpoints[-5:])\n",
    "    \n",
    "    # Keep every 2000th step checkpoint\n",
    "    for cp in checkpoints:\n",
    "        step = int(cp.split('_')[-1].split('.')[0])\n",
    "        if step % 2000 == 0:\n",
    "            keep_checkpoints.add(cp)\n",
    "    \n",
    "    # Delete others\n",
    "    for cp in checkpoints:\n",
    "        if cp not in keep_checkpoints:\n",
    "            print(f\"Removing old checkpoint: {cp}\")\n",
    "            log_message(f\"CLEANUP: Removed checkpoint {cp}\")\n",
    "            os.remove(cp)\n",
    "\n",
    "def train_model_production():\n",
    "    \"\"\"Production training loop - spot instance ready\"\"\"\n",
    "    \n",
    "    print(\"Starting production training...\")\n",
    "    \n",
    "    # Initialize logging\n",
    "    initialize_log()\n",
    "    log_message(\"Training started on EC2\")\n",
    "    \n",
    "    global shutdown_requested\n",
    "    \n",
    "    # Initial cleanup\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    # Check for existing checkpoints - auto-resume for spot instances\n",
    "    latest_checkpoint = find_latest_checkpoint()\n",
    "    start_step = 0\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    if latest_checkpoint:\n",
    "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "        start_step, start_epoch, best_loss = load_checkpoint_safe(\n",
    "            latest_checkpoint, model, optimizer, scheduler\n",
    "        )\n",
    "    else:\n",
    "        log_message(\"Starting training from scratch - no existing checkpoints found\")\n",
    "    \n",
    "    # Calculate starting position\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "    current_epoch = start_step // steps_per_epoch\n",
    "    current_step_in_epoch = start_step % steps_per_epoch\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    global_step = start_step\n",
    "    running_loss = 0.0\n",
    "    log_interval = 50  # Log every 50 steps\n",
    "    checkpoint_interval = 200  # Save every 200 steps\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    # Add table header to log\n",
    "    with open(\"training_log.md\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"| Step | Epoch | Loss | Learning Rate | GPU Memory |\\n\")\n",
    "        f.write(\"|------|-------|------|---------------|------------|\\n\")\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(start_epoch, 3):\n",
    "            if shutdown_requested:\n",
    "                break\n",
    "                \n",
    "            print(f\"Epoch {epoch + 1}/3\")\n",
    "            log_message(f\"EPOCH {epoch + 1} STARTED\")\n",
    "            \n",
    "            epoch_start_time = time.time()\n",
    "            epoch_loss = 0.0\n",
    "            epoch_steps = 0\n",
    "            \n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Skip batches if resuming mid-epoch\n",
    "            dataloader_iter = iter(train_dataloader)\n",
    "            for _ in range(current_step_in_epoch):\n",
    "                next(dataloader_iter)\n",
    "            \n",
    "            if epoch > start_epoch:\n",
    "                current_step_in_epoch = 0\n",
    "                dataloader_iter = iter(train_dataloader)\n",
    "            \n",
    "            progress_bar = tqdm(\n",
    "                dataloader_iter, \n",
    "                total=steps_per_epoch - current_step_in_epoch,\n",
    "                desc=f\"Epoch {epoch+1}\"\n",
    "            )\n",
    "            \n",
    "            for step, batch in enumerate(progress_bar, start=current_step_in_epoch):\n",
    "                if shutdown_requested:\n",
    "                    break\n",
    "                    \n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                labels = batch['labels'].to(device, non_blocking=True)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    use_cache=False\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                current_loss = loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "                \n",
    "                # Gradient accumulation and optimization\n",
    "                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    if global_step % 50 == 0:\n",
    "                        aggressive_memory_cleanup()\n",
    "                \n",
    "                # Update statistics\n",
    "                running_loss += current_loss\n",
    "                epoch_loss += current_loss\n",
    "                epoch_steps += 1\n",
    "                global_step += 1\n",
    "                \n",
    "                # Log progress every 50 steps\n",
    "                if global_step % log_interval == 0:\n",
    "                    avg_loss = running_loss / log_interval\n",
    "                    current_lr = scheduler.get_last_lr()[0]\n",
    "                    \n",
    "                    gpu_memory = 0\n",
    "                    if torch.cuda.is_available():\n",
    "                        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "                    \n",
    "                    # Store statistics\n",
    "                    training_stats['steps'].append(global_step)\n",
    "                    training_stats['losses'].append(avg_loss)\n",
    "                    training_stats['learning_rates'].append(current_lr)\n",
    "                    training_stats['epochs'].append(epoch + 1)\n",
    "                    training_stats['gpu_memory'].append(gpu_memory)\n",
    "                    \n",
    "                    print(f\"Step {global_step:,}/{TOTAL_STEPS:,} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e} | GPU: {gpu_memory:.1f}GB\")\n",
    "                    log_training_step(global_step, TOTAL_STEPS, avg_loss, current_lr, gpu_memory, epoch + 1)\n",
    "                    running_loss = 0.0\n",
    "                \n",
    "                # Checkpoint saving every 200 steps\n",
    "                if global_step % checkpoint_interval == 0:\n",
    "                    checkpoint_path = safe_checkpoint_save(\n",
    "                        model, optimizer, scheduler, \n",
    "                        global_step, epoch, current_loss\n",
    "                    )\n",
    "                    \n",
    "                    if checkpoint_path:\n",
    "                        cleanup_old_checkpoints()\n",
    "                \n",
    "                # Memory monitoring\n",
    "                if global_step % 100 == 0:\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "                        if memory_used > 20:\n",
    "                            aggressive_memory_cleanup()\n",
    "                \n",
    "                # Early stopping check\n",
    "                if global_step >= TOTAL_STEPS:\n",
    "                    break\n",
    "            \n",
    "            if shutdown_requested:\n",
    "                break\n",
    "                \n",
    "            # Epoch summary\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else 0\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1} complete | Loss: {avg_epoch_loss:.4f} | Time: {epoch_time/60:.1f}m\")\n",
    "            log_epoch_summary(epoch + 1, avg_epoch_loss, epoch_time/60, epoch_steps)\n",
    "            \n",
    "            # Save end-of-epoch checkpoint\n",
    "            safe_checkpoint_save(\n",
    "                model, optimizer, scheduler, \n",
    "                global_step, epoch + 1, avg_epoch_loss\n",
    "            )\n",
    "            cleanup_old_checkpoints()\n",
    "            \n",
    "            if global_step >= TOTAL_STEPS:\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted\")\n",
    "        log_message(\"Training interrupted by user\")\n",
    "        shutdown_requested = True\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        log_message(f\"ERROR: Training failed: {e}\")\n",
    "        shutdown_requested = True\n",
    "        raise\n",
    "    finally:\n",
    "        # Always save checkpoint on exit\n",
    "        if shutdown_requested or global_step > start_step:\n",
    "            print(\"Saving final checkpoint...\")\n",
    "            safe_checkpoint_save(model, optimizer, scheduler, global_step, epoch, current_loss)\n",
    "            cleanup_old_checkpoints()\n",
    "            log_message(f\"FINAL: Training ended at step {global_step:,}\")\n",
    "    \n",
    "    print(f\"Training completed. Total steps: {global_step:,}\")\n",
    "    log_message(f\"Training completed successfully. Total steps: {global_step:,}\")\n",
    "    \n",
    "    # Show available checkpoints\n",
    "    available_checkpoints = glob.glob(\"checkpoint_step_*.pt\")\n",
    "    if available_checkpoints:\n",
    "        print(f\"Available checkpoints: {len(available_checkpoints)} files\")\n",
    "        log_message(f\"Available checkpoints: {len(available_checkpoints)} files\")\n",
    "    \n",
    "    return training_stats\n",
    "\n",
    "# Check if all components are ready\n",
    "missing_components = []\n",
    "if 'model' not in globals() or model is None: missing_components.append(\"model\")\n",
    "if 'optimizer' not in globals() or optimizer is None: missing_components.append(\"optimizer\") \n",
    "if 'scheduler' not in globals() or scheduler is None: missing_components.append(\"scheduler\")\n",
    "if 'train_dataloader' not in globals() or train_dataloader is None: missing_components.append(\"dataloader\")\n",
    "\n",
    "if not missing_components:\n",
    "    print(\"All components ready for production training\")\n",
    "    print(\"Configuration:\")\n",
    "    print(f\"  - Log every 50 steps\")\n",
    "    print(f\"  - Save checkpoint every 200 steps\") \n",
    "    print(f\"  - Keep last 5 + every 2000th step checkpoint\")\n",
    "    print(f\"  - Spot instance interruption handling enabled\")\n",
    "    print(f\"  - Comprehensive logging to training_log.md\")\n",
    "    print(f\"  - Total steps: {TOTAL_STEPS:,}\")\n",
    "    \n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    try:\n",
    "        training_stats = train_model_production()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training cancelled\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "else:\n",
    "    print(f\"Missing components: {', '.join(missing_components)}\")\n",
    "    print(\"Please run previous cells first.\") "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
