{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Environment Setup and Imports\nimport subprocess\nimport sys\n\ndef install_package(package):\n    try:\n        __import__(package)\n    except ImportError:\n        print(f\"Installing {package}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n# Required packages\nrequired_packages = [\n    \"transformers>=4.36.0\",\n    \"datasets>=2.14.0\", \n    \"torch>=2.0.0\",\n    \"accelerate>=0.24.0\",\n    \"flash-attn>=2.3.0\",\n    \"wandb\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"numpy\",\n    \"tqdm\"\n]\n\n# Install packages if needed (uncomment if running first time)\n# for package in required_packages:\n#     install_package(package.split(\">=\")[0])\n\n# Core imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Transformers and datasets\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    AutoConfig,\n    get_linear_schedule_with_warmup,\n    get_cosine_schedule_with_warmup,\n    TrainingArguments,\n    set_seed\n)\nfrom datasets import load_dataset\n\n# Utilities\nimport os\nimport json\nimport time\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"✅ Environment setup complete\") ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: GPU Check and Random Seed Setup\ndef setup_device_and_seed(seed=42):\n    \"\"\"Setup device, log GPU info, and set random seeds\"\"\"\n    \n    # Set random seeds for reproducibility\n    set_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \n    # Ensure deterministic behavior\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    # Check CUDA availability\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        num_gpus = torch.cuda.device_count()\n    else:\n        device = torch.device(\"cpu\")\n        num_gpus = 0\n    \n    return device, num_gpus\n\n# Setup device and seeds\nSEED = 42\ndevice, num_gpus = setup_device_and_seed(SEED)\n\n# Training configuration\nMULTI_GPU = num_gpus > 1\n\n# Clear GPU cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(f\"✅ Device setup complete - Using {device} with {num_gpus} GPUs\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Load and Explore Dataset (Cosmopedia 100k)\ndef load_and_explore_dataset():\n    \"\"\"Load Cosmopedia 100k dataset\"\"\"\n    \n    try:\n        ds = load_dataset(\"HuggingFaceTB/cosmopedia-100k\", split=\"train\")\n        return ds\n    except Exception as e:\n        print(f\"❌ Error loading dataset: {e}\")\n        return None\n\n# Load the dataset\ndataset = load_and_explore_dataset()\n\n# Store dataset info for later use\nif dataset:\n    DATASET_SIZE = len(dataset)\n    print(f\"✅ Dataset loaded - {DATASET_SIZE:,} samples\")\nelse:\n    print(\"❌ Failed to load dataset. Please check your connection and try again.\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Tokenizer - Existing Tokenizer\ndef load_smollm2_tokenizer():\n    \"\"\"Load the existing SmolLM2 tokenizer\"\"\"\n    \n    model_id = \"HuggingFaceTB/SmolLM2-1.7B\"\n    \n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        \n        # Add padding token if it doesn't exist\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            \n        return tokenizer\n        \n    except Exception as e:\n        print(f\"❌ Error loading tokenizer: {e}\")\n        return None\n\n# Load the tokenizer\ntokenizer = load_smollm2_tokenizer()\n\nif tokenizer:\n    # Set sequence length for training\n    MAX_SEQ_LENGTH = 1024\n    print(\"✅ Tokenizer loaded successfully\")\nelse:\n    print(\"❌ Failed to load tokenizer. Please check your connection and model ID.\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Dataset Preprocessing - ULTRA OPTIMIZED (Minimal Memory)\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nclass CosmopediaDataset(Dataset):\n    \"\"\"Ultra Memory-Optimized Dataset\"\"\"\n    \n    def __init__(self, dataset, tokenizer, max_length=256):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # Get text from dataset\n        text = self.dataset[idx]['text']\n        \n        # Tokenize with aggressive truncation\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        # Extract input_ids and attention_mask\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze()\n        \n        # For causal language modeling, labels are the same as input_ids\n        labels = input_ids.clone()\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels\n        }\n\ndef preprocess_dataset_ultra_optimized(dataset, tokenizer, max_length=256, batch_size=1):\n    \"\"\"Ultra memory-optimized preprocessing\"\"\"\n    \n    if dataset is None or tokenizer is None:\n        print(\"❌ Dataset or tokenizer is None. Please load them first.\")\n        return None, None\n    \n    # Create custom dataset\n    torch_dataset = CosmopediaDataset(dataset, tokenizer, max_length)\n    \n    # Create DataLoader with ultra optimization\n    dataloader = DataLoader(\n        torch_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=True,\n        persistent_workers=False\n    )\n    \n    return torch_dataset, dataloader\n\n# ULTRA OPTIMIZED Configuration\nBATCH_SIZE = 1\nGRADIENT_ACCUMULATION_STEPS = 64\nMAX_SEQ_LENGTH_ULTRA = 256\n\n# Calculate effective batch size\nEFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * max(1, num_gpus)\n\n# Clear GPU memory aggressively\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n\n# Preprocess dataset with ultra settings\ntrain_dataset, train_dataloader = preprocess_dataset_ultra_optimized(\n    dataset, \n    tokenizer, \n    max_length=MAX_SEQ_LENGTH_ULTRA, \n    batch_size=BATCH_SIZE\n)\n\nif train_dataloader:\n    # Calculate final training statistics\n    STEPS_PER_EPOCH = len(train_dataloader)\n    TOTAL_STEPS = STEPS_PER_EPOCH * 3  # 3 epochs\n    \n    print(f\"✅ Dataset preprocessing complete - {len(train_dataloader):,} batches\")\nelse:\n    print(\"❌ Failed to create DataLoader. Please check your setup.\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Model Configuration and Initialization (NO MIXED PRECISION VERSION)\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\ndef setup_model_and_training_components_no_amp():\n    \"\"\"Initialize SmolLM2 model with random weights and setup training components - NO MIXED PRECISION\"\"\"\n    \n    # Clear GPU cache before model initialization\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Load SmolLM2 configuration\n    model_id = \"HuggingFaceTB/SmolLM2-1.7B\"\n    \n    try:\n        # Load config only (not weights)\n        config = AutoConfig.from_pretrained(model_id)\n        \n        # MEMORY OPTIMIZATION: Enable gradient checkpointing\n        config.use_cache = False\n        config.gradient_checkpointing = True\n        \n        # Enable Flash Attention 2 if available\n        try:\n            config.use_flash_attention_2 = True\n        except:\n            pass\n        \n    except Exception as e:\n        print(f\"❌ Error loading config: {e}\")\n        return None, None, None, None, None\n    \n    # Initialize model with random weights\n    try:\n        model = AutoModelForCausalLM.from_config(config)\n        model.gradient_checkpointing_enable()\n        \n        # Count parameters\n        total_params = sum(p.numel() for p in model.parameters())\n        \n    except Exception as e:\n        print(f\"❌ Error initializing model: {e}\")\n        return None, None, None, None, None\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Setup optimizer (AdamW)\n    learning_rate = 3e-5\n    weight_decay = 0.01\n    \n    # No weight decay for bias and layer norm parameters\n    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    \n    optimizer = torch.optim.AdamW(\n        optimizer_grouped_parameters,\n        lr=learning_rate,\n        betas=(0.9, 0.95),\n        eps=1e-8,\n        foreach=False\n    )\n    \n    # Setup learning rate scheduler\n    num_warmup_steps = int(0.1 * TOTAL_STEPS)  # 10% warmup\n    \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=TOTAL_STEPS\n    )\n    \n    # NO MIXED PRECISION - Use None for scaler\n    scaler = None\n    \n    # Final memory check\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    return model, optimizer, scheduler, scaler, config\n\n# Initialize model and training components WITHOUT mixed precision\nmodel, optimizer, scheduler, scaler, model_config = setup_model_and_training_components_no_amp()\n\nif model is not None:\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"✅ Model initialized - {total_params/1e9:.2f}B parameters\")\nelse:\n    print(\"❌ Failed to initialize model. Please check your setup.\")\n\n# Gradient clipping value\nMAX_GRAD_NORM = 1.0 ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Ultra-Safe Training Loop with Robust Checkpointing\nimport glob\nimport os\nimport gc\nfrom pathlib import Path\nimport shutil\n\n# Set environment variables for maximum stability\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Training tracking variables\ntraining_stats = {\n    'steps': [],\n    'losses': [],\n    'learning_rates': [],\n    'epochs': [],\n    'gpu_memory': []\n}\n\ndef aggressive_memory_cleanup():\n    \"\"\"Ultra-aggressive memory cleanup\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    gc.collect()\n\ndef safe_checkpoint_save(model, optimizer, scheduler, step, epoch, loss, checkpoint_dir=\"./checkpoints\"):\n    \"\"\"Ultra-safe checkpoint saving with corruption prevention\"\"\"\n    \n    Path(checkpoint_dir).mkdir(exist_ok=True)\n    aggressive_memory_cleanup()\n    \n    checkpoint = {\n        'step': step,\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'loss': loss,\n        'training_stats': training_stats\n    }\n    \n    checkpoint_path = f\"{checkpoint_dir}/checkpoint_step_{step}.pt\"\n    temp_path = f\"{checkpoint_dir}/temp_checkpoint_step_{step}.pt\"\n    \n    try:\n        torch.save(checkpoint, temp_path)\n        test_load = torch.load(temp_path, map_location='cpu')\n        if 'step' in test_load and test_load['step'] == step:\n            shutil.move(temp_path, checkpoint_path)\n        else:\n            raise Exception(\"Checkpoint verification failed\")\n    except Exception as e:\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n        return None\n    \n    aggressive_memory_cleanup()\n    return checkpoint_path\n\ndef load_checkpoint_safe(checkpoint_path, model, optimizer, scheduler):\n    \"\"\"Safe checkpoint loading with verification\"\"\"\n    \n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        \n        if 'model_state_dict' not in checkpoint:\n            raise Exception(\"Invalid checkpoint format\")\n        \n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        start_step = checkpoint['step']\n        start_epoch = checkpoint['epoch']\n        best_loss = checkpoint['loss']\n        \n        global training_stats\n        training_stats = checkpoint.get('training_stats', training_stats)\n        \n        print(f\"📂 Checkpoint loaded - resuming from step {start_step}\")\n        return start_step, start_epoch, best_loss\n        \n    except Exception as e:\n        print(f\"❌ Error loading checkpoint: {e}\")\n        return 0, 0, float('inf')\n\ndef find_latest_checkpoint(checkpoint_dir=\"./checkpoints\"):\n    \"\"\"Find the latest valid checkpoint\"\"\"\n    \n    if not os.path.exists(checkpoint_dir):\n        return None\n        \n    checkpoints = glob.glob(f\"{checkpoint_dir}/checkpoint_step_*.pt\")\n    if not checkpoints:\n        return None\n        \n    valid_checkpoints = []\n    for cp in checkpoints:\n        try:\n            test = torch.load(cp, map_location='cpu')\n            if 'step' in test:\n                valid_checkpoints.append(cp)\n        except:\n            os.remove(cp)\n    \n    if not valid_checkpoints:\n        return None\n        \n    valid_checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n    return valid_checkpoints[-1]\n\ndef train_model_ultra_safe():\n    \"\"\"Ultra-safe training with aggressive memory management\"\"\"\n    \n    print(\"🚀 Starting training...\")\n    \n    # Initial aggressive cleanup\n    aggressive_memory_cleanup()\n    \n    # Check for existing checkpoints\n    latest_checkpoint = find_latest_checkpoint()\n    start_step = 0\n    start_epoch = 0\n    best_loss = float('inf')\n    \n    if latest_checkpoint:\n        response = input(f\"Found checkpoint: {latest_checkpoint}. Resume? (y/n): \")\n        if response.lower() == 'y':\n            start_step, start_epoch, best_loss = load_checkpoint_safe(\n                latest_checkpoint, model, optimizer, scheduler\n            )\n    \n    # Calculate starting position\n    steps_per_epoch = len(train_dataloader)\n    current_epoch = start_step // steps_per_epoch\n    current_step_in_epoch = start_step % steps_per_epoch\n    \n    # Training loop\n    model.train()\n    global_step = start_step\n    running_loss = 0.0\n    log_interval = 50\n    current_loss = 0.0\n    \n    try:\n        for epoch in range(start_epoch, 3):\n            print(f\"\\nEpoch {epoch + 1}/3\")\n            \n            epoch_start_time = time.time()\n            epoch_loss = 0.0\n            epoch_steps = 0\n            \n            aggressive_memory_cleanup()\n            \n            # Skip batches if resuming mid-epoch\n            dataloader_iter = iter(train_dataloader)\n            for _ in range(current_step_in_epoch):\n                next(dataloader_iter)\n            \n            if epoch > start_epoch:\n                current_step_in_epoch = 0\n                dataloader_iter = iter(train_dataloader)\n            \n            progress_bar = tqdm(\n                dataloader_iter, \n                total=steps_per_epoch - current_step_in_epoch,\n                desc=f\"Epoch {epoch+1}\"\n            )\n            \n            for step, batch in enumerate(progress_bar, start=current_step_in_epoch):\n                # Move batch to device\n                input_ids = batch['input_ids'].to(device, non_blocking=True)\n                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n                labels = batch['labels'].to(device, non_blocking=True)\n                \n                # Forward pass (FP32)\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels,\n                    use_cache=False\n                )\n                loss = outputs.loss\n                loss = loss / GRADIENT_ACCUMULATION_STEPS\n                \n                # Backward pass\n                loss.backward()\n                current_loss = loss.item() * GRADIENT_ACCUMULATION_STEPS\n                \n                # Gradient accumulation and optimization\n                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n                    optimizer.step()\n                    scheduler.step()\n                    optimizer.zero_grad()\n                    \n                    if global_step % 10 == 0:\n                        aggressive_memory_cleanup()\n                \n                # Update statistics\n                running_loss += current_loss\n                epoch_loss += current_loss\n                epoch_steps += 1\n                global_step += 1\n                \n                # Log progress\n                if global_step % log_interval == 0:\n                    avg_loss = running_loss / log_interval\n                    current_lr = scheduler.get_last_lr()[0]\n                    \n                    gpu_memory = 0\n                    if torch.cuda.is_available():\n                        gpu_memory = torch.cuda.memory_allocated() / 1e9\n                    \n                    # Store statistics\n                    training_stats['steps'].append(global_step)\n                    training_stats['losses'].append(avg_loss)\n                    training_stats['learning_rates'].append(current_lr)\n                    training_stats['epochs'].append(epoch + 1)\n                    training_stats['gpu_memory'].append(gpu_memory)\n                    \n                    print(f\"Step {global_step:,}/{TOTAL_STEPS:,} - Loss: {avg_loss:.4f}\")\n                    running_loss = 0.0\n                \n                # Checkpoint saving every 200 steps\n                if global_step % 200 == 0:\n                    checkpoint_path = safe_checkpoint_save(\n                        model, optimizer, scheduler, \n                        global_step, epoch, current_loss\n                    )\n                    \n                    if checkpoint_path:\n                        # Keep only last 2 checkpoints\n                        checkpoints = glob.glob(\"./checkpoints/checkpoint_step_*.pt\")\n                        if len(checkpoints) > 2:\n                            checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n                            for old_checkpoint in checkpoints[:-2]:\n                                os.remove(old_checkpoint)\n                \n                # Memory monitoring\n                if global_step % 100 == 0:\n                    if torch.cuda.is_available():\n                        memory_used = torch.cuda.memory_allocated() / 1e9\n                        if memory_used > 12:\n                            aggressive_memory_cleanup()\n                \n                # Early stopping check\n                if global_step >= TOTAL_STEPS:\n                    break\n            \n            # Epoch summary\n            epoch_time = time.time() - epoch_start_time\n            avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else 0\n            \n            print(f\"Epoch {epoch + 1} complete - Avg Loss: {avg_epoch_loss:.4f} - Time: {epoch_time/60:.1f}m\")\n            \n            # Save end-of-epoch checkpoint\n            safe_checkpoint_save(\n                model, optimizer, scheduler, \n                global_step, epoch + 1, avg_epoch_loss\n            )\n            \n            if global_step >= TOTAL_STEPS:\n                break\n    \n    except KeyboardInterrupt:\n        print(\"Training interrupted by user\")\n        safe_checkpoint_save(model, optimizer, scheduler, global_step, epoch, current_loss)\n    except Exception as e:\n        print(f\"Training error: {e}\")\n        safe_checkpoint_save(model, optimizer, scheduler, global_step, epoch, current_loss)\n        raise\n    \n    print(f\"Training completed! Total steps: {global_step:,}\")\n    return training_stats\n\n# Check if all components are ready\nmissing_components = []\nif 'model' not in globals() or model is None: missing_components.append(\"model\")\nif 'optimizer' not in globals() or optimizer is None: missing_components.append(\"optimizer\") \nif 'scheduler' not in globals() or scheduler is None: missing_components.append(\"scheduler\")\nif 'train_dataloader' not in globals() or train_dataloader is None: missing_components.append(\"dataloader\")\n\nif not missing_components:\n    print(\"✅ All components ready for training\")\n    \n    aggressive_memory_cleanup()\n    \n    try:\n        training_stats = train_model_ultra_safe()\n    except KeyboardInterrupt:\n        print(\"Training cancelled by user\")\n    except Exception as e:\n        print(f\"Training failed: {e}\")\nelse:\n    print(f\"❌ Missing components: {', '.join(missing_components)}\")\n    print(\"Please run previous cells first.\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}